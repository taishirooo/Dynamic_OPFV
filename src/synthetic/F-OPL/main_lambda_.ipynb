{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from synthetic_time import SyntheticBanditWithTimeDataset\n",
    "from utils import show_hyperparameters\n",
    "import conf\n",
    "from opl import OPL\n",
    "from logging import getLogger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:The current working directory is /Users/s23599/document/research/Sony-Non-Stationary-OPE-OPL/icml2024-opfv-change-name/src/synthetic/F-OPL\n"
     ]
    }
   ],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.info(f\"The current working directory is {Path().cwd()}\")\n",
    "\n",
    "# log path\n",
    "log_path = Path(\"./varying_lambda_data\")\n",
    "df_path = log_path / \"df\"\n",
    "df_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# START hyperparameters #################\n",
      "### About Seeds and Number of Samples ###\n",
      "number of seeds = 20\n",
      "number of training samples (n) = 8000\n",
      "number of test samples = 10000\n",
      "\n",
      "### About Time Structure ###\n",
      "number of true time structures for reward (|C_r|) = 8\n",
      "strength of time structure for reward (lambda) = 0.5\n",
      "\n",
      "### About OPL ###\n",
      "number of epochs = 25\n",
      "batch size = 32\n",
      "number of the samples of time when we learn a policy for each batch = 50\n",
      "\n",
      "### About Prognosticator ###\n",
      "list of time features for Prognosticator = [<function fourier_scalar at 0x298fe65e0>]\n",
      "optimality of the data driven feature selection for Prognosticator = True\n",
      "number of time features for Prognosticator = 3\n",
      "list of the numbers of time features for Prognosticator = range(3, 8, 2)\n",
      "\n",
      "### About Logged Data Collection Period and evaluation Period ###\n",
      "time when we start collecting the logged data = 2022-01-01 00:00:00\n",
      "time when we finish collecting the logged data = 2022-12-31 23:59:59\n",
      "time when we start evaluation a target policy = 2023-01-01 00:00:00\n",
      "time when we finish evaluation a target policy = 2024-01-01 00:00:00\n",
      "future time = 2024-01-01 00:00:00\n",
      "\n",
      "### About Parameters for Data Generating Process ###\n",
      "number of actions (|A|) = 10\n",
      "dimension of context (d_x) = 10\n",
      "number of users = None\n",
      "behavior policy optimality (beta) = 0.1\n",
      "target policy optimality (epsilon) = 0\n",
      "\n",
      "### About Varying Parameters ###\n",
      "list of the numbers of training samples (n) = [1000, 2000, 4000, 8000]\n",
      "list of the strengths of time structure for reward (lambda) = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "list of the numbers of candidate time structures for reward = range(2, 9)\n",
      "################# END hyperparameters #################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lambda_ratio=0.0: 100%|██████████| 20/20 [8:05:30<00:00, 1456.51s/it]  \n",
      "lambda_ratio=0.2: 100%|██████████| 20/20 [5:36:51<00:00, 1010.57s/it]  \n",
      "lambda_ratio=0.4: 100%|██████████| 20/20 [2:33:41<00:00, 461.10s/it]  \n",
      "lambda_ratio=0.6: 100%|██████████| 20/20 [1:16:22<00:00, 229.14s/it]\n",
      "lambda_ratio=0.8: 100%|██████████| 20/20 [59:36<00:00, 178.80s/it]\n",
      "lambda_ratio=1.0: 100%|██████████| 20/20 [1:00:20<00:00, 181.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 1173.4567830522856 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "x = \"lambda_ratio\"\n",
    "xlabel = r\"$\\lambda$\"\n",
    "xticklabels = conf.lambda_ratio_list\n",
    "\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(conf.random_state)\n",
    "\n",
    "result_df_list = []\n",
    "\n",
    "# Test Data \n",
    "# Obtain the unix time when we start the evaluation of a target policy\n",
    "time_at_evaluation_start = conf.time_at_evaluation_start\n",
    "\n",
    "# Calculate the number of days in one cycle of given time structure function \\phi(t)\n",
    "NUM_DAYS_IN_ONE_CYCLE = 365\n",
    "\n",
    "# Determine the unix time when we end the evaluation of a target policy \n",
    "time_at_evaluation_end_datetime = datetime.datetime.fromtimestamp(time_at_evaluation_start) + datetime.timedelta(days=NUM_DAYS_IN_ONE_CYCLE * conf.num_cycles_in_evaluation_period)\n",
    "time_at_evaluation_end = int(datetime.datetime.timestamp(time_at_evaluation_end_datetime))\n",
    "\n",
    "# Show hyperparameters\n",
    "show_hyperparameters(\n",
    "    time_at_evaluation_start = time_at_evaluation_start, \n",
    "    time_at_evaluation_end = time_at_evaluation_end, \n",
    "    flag_show_time_at_evaluation = True, \n",
    "    time_at_evaluation_list = None,\n",
    ")\n",
    "\n",
    "result_df_list = []\n",
    "for lambda_ratio in conf.lambda_ratio_list:\n",
    "    test_policy_value_list = []\n",
    "\n",
    "    dataset = SyntheticBanditWithTimeDataset(\n",
    "        n_actions=conf.n_actions,  # Number of Actions |A|\n",
    "        dim_context=conf.dim_context, # Dimension of the context d_x\n",
    "        n_users=conf.n_users, # number of users \n",
    "        t_oldest = conf.t_oldest, # time when we start collecting the logged data\n",
    "        t_now = conf.t_now,  # time when we finish collecting the logged data\n",
    "        t_future = conf.t_future, # Future time\n",
    "        beta = conf.beta, # optimality of the behavior policy\n",
    "        reward_std = conf.reward_std, # standard deviation of reward\n",
    "        num_time_structure=conf.num_time_structure_for_logged_data, # the true number of time structure for reward\n",
    "        num_time_structure_for_context=conf.num_time_structure_for_context, \n",
    "        lambda_ratio = lambda_ratio, # strength of the influence of the time structure for reward\n",
    "        alpha_ratio = conf.alpha_ratio, # strength of the influence of the time structure for context\n",
    "        flag_simple_reward = conf.flag_simple_reward, # if expected reward function is simple or not\n",
    "        sample_non_stationary_context = False, # if the context is non-stationary or not\n",
    "        g_coef=conf.g_coef, # parameter for generating g(x, phi(t), a)\n",
    "        h_coef=conf.h_coef, # parameter for generating h(x, t, a)\n",
    "        p_1_coef = conf.p_1_coef, # parameter for generating the part of non-staitonary context affected by time structure for context\n",
    "        p_2_coef = conf.p_2_coef, # parameter for generating the part of non-staitonary context not affected by time structure for context\n",
    "        random_state=conf.random_state, # random state\n",
    "    )\n",
    "\n",
    "    random_ = check_random_state(conf.random_state)\n",
    "    # Sample the time at evaluation from given distribution (uniform)\n",
    "    time_at_evaluation_vec = random_.uniform(time_at_evaluation_start, time_at_evaluation_end, size=conf.num_test).astype(int)\n",
    "\n",
    "\n",
    "    ### test bandit data is used to approximate the ground-truth policy value\n",
    "    dataset_test = dataset.obtain_batch_bandit_feedback(\n",
    "        n_rounds=conf.num_test, \n",
    "        evaluation_mode=True, \n",
    "        time_at_evaluation_vec=time_at_evaluation_vec, \n",
    "        random_state_for_sampling = conf.random_state, \n",
    "    )\n",
    "\n",
    "    for _ in tqdm(range(conf.n_seeds), desc=f\"{x}={lambda_ratio}\"):\n",
    "        ## generate training data\n",
    "        dataset_train = dataset.obtain_batch_bandit_feedback(\n",
    "            n_rounds=conf.num_train, \n",
    "            evaluation_mode=False, \n",
    "            random_state_for_sampling=_\n",
    "        )\n",
    "\n",
    "\n",
    "        true_value_of_learned_policies, pi_0_value = OPL(\n",
    "            dataset = dataset, \n",
    "            dataset_test = dataset_test, \n",
    "            dataset_train = dataset_train, \n",
    "            time_at_evaluation_start = time_at_evaluation_start, \n",
    "            time_at_evaluation_end = time_at_evaluation_end, \n",
    "            round = _, \n",
    "            flag_plot_loss = conf.flag_plot_loss, \n",
    "            flag_plot_value = conf.flag_plot_value, \n",
    "            num_time_structure_for_OPFV_reward = conf.num_true_time_structure_for_OPFV_reward, \n",
    "            max_iter = conf.max_iter, \n",
    "            batch_size = conf.batch_size, \n",
    "            num_time_learn = conf.num_time_learn, \n",
    "        )\n",
    "\n",
    "        test_policy_value_list.append(true_value_of_learned_policies)\n",
    "\n",
    "    ## summarize results\n",
    "    result_df = DataFrame(test_policy_value_list).stack().reset_index(1)\\\n",
    "        .rename(columns={\"level_1\": \"method\", 0: \"value\"})\n",
    "    result_df[f\"{x}\"] = lambda_ratio\n",
    "    result_df[\"pi_0_value\"] = pi_0_value\n",
    "    result_df[\"rel_value\"] = result_df[\"value\"] / pi_0_value\n",
    "    result_df_list.append(result_df)\n",
    "result_df_data = pd.concat(result_df_list).reset_index(level=0)\n",
    "result_df_data.to_csv(df_path / \"result_df_data.csv\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"execution time: {elapsed_time / 60} mins\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
