{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import check_random_state\n",
    "from synthetic_time import SyntheticBanditWithTimeDataset\n",
    "from utils import show_hyperparameters\n",
    "import conf\n",
    "from opl import OPL\n",
    "from logging import getLogger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:The current working directory is /Users/s23599/document/research/Sony-Non-Stationary-OPE-OPL/icml2024-opfv-change-name/src/synthetic/F-OPL\n"
     ]
    }
   ],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.info(f\"The current working directory is {Path().cwd()}\")\n",
    "\n",
    "# log path\n",
    "log_path = Path(\"./varying_target_time_data\")\n",
    "df_path = log_path / \"df\"\n",
    "df_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# START hyperparameters #################\n",
      "### About Seeds and Number of Samples ###\n",
      "number of seeds = 20\n",
      "number of training samples (n) = 8000\n",
      "number of test samples = 10000\n",
      "\n",
      "### About Time Structure ###\n",
      "number of true time structures for reward (|C_r|) = 8\n",
      "strength of time structure for reward (lambda) = 0.5\n",
      "\n",
      "### About OPL ###\n",
      "number of epochs = 25\n",
      "batch size = 32\n",
      "number of the samples of time when we learn a policy for each batch = 50\n",
      "\n",
      "### About Prognosticator ###\n",
      "list of time features for Prognosticator = [<function fourier_scalar at 0x15ac975e0>]\n",
      "optimality of the data driven feature selection for Prognosticator = True\n",
      "number of time features for Prognosticator = 3\n",
      "list of the numbers of time features for Prognosticator = range(3, 8, 2)\n",
      "\n",
      "### About Logged Data Collection Period and evaluation Period ###\n",
      "time when we start collecting the logged data = 2022-01-01 00:00:00\n",
      "time when we finish collecting the logged data = 2022-12-31 23:59:59\n",
      "future time = 2024-01-01 00:00:00\n",
      "\n",
      "### About Parameters for Data Generating Process ###\n",
      "number of actions (|A|) = 10\n",
      "dimension of context (d_x) = 10\n",
      "number of users = None\n",
      "behavior policy optimality (beta) = 0.1\n",
      "target policy optimality (epsilon) = 0\n",
      "\n",
      "### About Varying Parameters ###\n",
      "list of the numbers of training samples (n) = [1000, 2000, 4000, 8000]\n",
      "list of the strengths of time structure for reward (lambda) = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "list of the numbers of candidate time structures for reward = range(2, 9)\n",
      "list of the time at evaluation = [datetime.datetime(2023, 2, 14, 23, 59, 59)\n",
      " datetime.datetime(2023, 4, 1, 23, 59, 59)\n",
      " datetime.datetime(2023, 5, 16, 23, 59, 59)\n",
      " datetime.datetime(2023, 7, 1, 23, 59, 59)\n",
      " datetime.datetime(2023, 8, 16, 23, 59, 59)\n",
      " datetime.datetime(2023, 9, 30, 23, 59, 59)\n",
      " datetime.datetime(2023, 11, 15, 23, 59, 59)\n",
      " datetime.datetime(2023, 12, 31, 23, 59, 59)]\n",
      "################# END hyperparameters #################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time_at_evaluation=1~45: 100%|██████████| 20/20 [7:57:55<00:00, 1433.76s/it]\n",
      "time_at_evaluation=46~91: 100%|██████████| 20/20 [5:32:29<00:00, 997.49s/it]\n",
      "time_at_evaluation=92~136: 100%|██████████| 20/20 [2:34:09<00:00, 462.49s/it]\n",
      "time_at_evaluation=137~182: 100%|██████████| 20/20 [1:16:30<00:00, 229.55s/it]\n",
      "time_at_evaluation=183~228: 100%|██████████| 20/20 [55:18<00:00, 165.92s/it]\n",
      "time_at_evaluation=229~273: 100%|██████████| 20/20 [55:15<00:00, 165.76s/it]\n",
      "time_at_evaluation=274~319: 100%|██████████| 20/20 [52:47<00:00, 158.39s/it]\n",
      "time_at_evaluation=320~365: 100%|██████████| 20/20 [52:28<00:00, 157.41s/it]\n",
      "100%|██████████| 8/8 [20:58:08<00:00, 9436.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 1258.1422306338945 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "x = \"time_at_evaluation\"\n",
    "xlabel = \"target time (days later)\"\n",
    "\n",
    "time_at_evaluation_list = []\n",
    "x_ticks_list_single = []\n",
    "\n",
    "NUM_DAYS_IN_ONE_CYCLE = 365\n",
    "\n",
    "for i in range(conf.num_time_at_evaluation):\n",
    "    t_at_evaluation_datetime = datetime.datetime.fromtimestamp(conf.t_now) + datetime.timedelta(days=((i+1) * NUM_DAYS_IN_ONE_CYCLE // conf.num_time_structure_for_logged_data))\n",
    "    t_at_evaluation = int(datetime.datetime.timestamp(t_at_evaluation_datetime))\n",
    "    time_at_evaluation_list.append(t_at_evaluation)\n",
    "    x_ticks_list_single.append((i+1) * 365 // conf.num_time_structure_for_logged_data)\n",
    "\n",
    "x_ticks_list = []\n",
    "\n",
    "for i in range(len(x_ticks_list_single)):\n",
    "    if i != 0:\n",
    "        x_ticks_list.append(f\"{x_ticks_list_single[i - 1] + 1}~{x_ticks_list_single[i]}\")\n",
    "    else:\n",
    "        x_ticks_list.append(f\"1~{x_ticks_list_single[i]}\")\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(conf.random_state)\n",
    "\n",
    "result_df_list = []\n",
    "\n",
    "\n",
    "# Show hyperparameters\n",
    "show_hyperparameters(\n",
    "    time_at_evaluation_start = None, \n",
    "    time_at_evaluation_end = None, \n",
    "    flag_show_time_at_evaluation = False, \n",
    "    time_at_evaluation_list = time_at_evaluation_list,\n",
    ")\n",
    "\n",
    "result_df_list = []\n",
    "for i in tqdm(range(len(time_at_evaluation_list))):\n",
    "    test_policy_value_list = []\n",
    "\n",
    "    dataset = SyntheticBanditWithTimeDataset(\n",
    "        n_actions=conf.n_actions,  # Number of Actions |A|\n",
    "        dim_context=conf.dim_context, # Dimension of the context d_x\n",
    "        n_users=conf.n_users, # number of users \n",
    "        t_oldest = conf.t_oldest, # time when we start collecting the logged data\n",
    "        t_now = conf.t_now,  # time when we finish collecting the logged data\n",
    "        t_future = conf.t_future, # Future time\n",
    "        beta = conf.beta, # optimality of the behavior policy\n",
    "        reward_std = conf.reward_std, # standard deviation of reward\n",
    "        num_time_structure=conf.num_time_structure_for_logged_data, # the true number of time structure for reward\n",
    "        num_time_structure_for_context=conf.num_time_structure_for_context, \n",
    "        lambda_ratio = conf.lambda_ratio, # strength of the influence of the time structure for reward\n",
    "        alpha_ratio = conf.alpha_ratio, # strength of the influence of the time structure for context\n",
    "        flag_simple_reward = conf.flag_simple_reward, # if expected reward function is simple or not\n",
    "        sample_non_stationary_context = False, # if the context is non-stationary or not\n",
    "        g_coef=conf.g_coef, # parameter for generating g(x, phi(t), a)\n",
    "        h_coef=conf.h_coef, # parameter for generating h(x, t, a)\n",
    "        p_1_coef = conf.p_1_coef, # parameter for generating the part of non-staitonary context affected by time structure for context\n",
    "        p_2_coef = conf.p_2_coef, # parameter for generating the part of non-staitonary context not affected by time structure for context\n",
    "        random_state=conf.random_state, # random state\n",
    "    )\n",
    "\n",
    "    time_at_evaluation_start = time_at_evaluation_list[i]\n",
    "    time_at_evaluation_end = time_at_evaluation_list[i]\n",
    "\n",
    "    if i != 0:\n",
    "        time_at_evaluation_start = time_at_evaluation_list[i - 1] + 1\n",
    "        time_at_evaluation_end = time_at_evaluation_list[i]\n",
    "    else:\n",
    "        time_at_evaluation_start = dataset.t_now + 1\n",
    "        time_at_evaluation_end = time_at_evaluation_list[i]\n",
    "\n",
    "    \n",
    "    random_ = check_random_state(conf.random_state + i)\n",
    "\n",
    "    # Sample the time at evaluation from given distribution (uniform)\n",
    "    time_at_evaluation_vec = random_.uniform(time_at_evaluation_start, time_at_evaluation_end, size=conf.num_test).astype(int)\n",
    "\n",
    "\n",
    "    ### test bandit data is used to approximate the ground-truth policy value\n",
    "    dataset_test = dataset.obtain_batch_bandit_feedback(\n",
    "        n_rounds=conf.num_test, \n",
    "        evaluation_mode=True, \n",
    "        time_at_evaluation_vec=time_at_evaluation_vec, \n",
    "        random_state_for_sampling=conf.random_state + i\n",
    "    )\n",
    "\n",
    "    for _ in tqdm(range(conf.n_seeds), desc=f\"{x}={x_ticks_list[i]}\"):\n",
    "        ## generate training data\n",
    "        dataset_train = dataset.obtain_batch_bandit_feedback(\n",
    "            n_rounds=conf.num_train, \n",
    "            evaluation_mode=False, \n",
    "            random_state_for_sampling=_\n",
    "        )\n",
    "\n",
    "        true_value_of_learned_policies, pi_0_value = OPL(\n",
    "            dataset = dataset, \n",
    "            dataset_test = dataset_test, \n",
    "            dataset_train = dataset_train, \n",
    "            time_at_evaluation_start = time_at_evaluation_start, \n",
    "            time_at_evaluation_end = time_at_evaluation_end, \n",
    "            round = _, \n",
    "            flag_plot_loss = conf.flag_plot_loss, \n",
    "            flag_plot_value = conf.flag_plot_value, \n",
    "            num_time_structure_for_OPFV_reward = conf.num_true_time_structure_for_OPFV_reward, \n",
    "            n_actions = conf.n_actions, \n",
    "            dim_context = conf.dim_context, \n",
    "            max_iter = conf.max_iter, \n",
    "            batch_size = conf.batch_size, \n",
    "            num_time_learn = conf.num_time_learn, \n",
    "        )\n",
    "\n",
    "        test_policy_value_list.append(true_value_of_learned_policies)\n",
    "\n",
    "    ## summarize results\n",
    "    result_df = DataFrame(test_policy_value_list).stack().reset_index(1)\\\n",
    "        .rename(columns={\"level_1\": \"method\", 0: \"value\"})\n",
    "    result_df[f\"{x}\"] = x_ticks_list_single[i]\n",
    "    result_df[\"pi_0_value\"] = pi_0_value\n",
    "    result_df[\"rel_value\"] = result_df[\"value\"] / pi_0_value\n",
    "    result_df_list.append(result_df)\n",
    "result_df_data = pd.concat(result_df_list).reset_index(level=0)\n",
    "result_df_data.to_csv(df_path / \"result_df_data.csv\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"execution time: {elapsed_time / 60} mins\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
