{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from synthetic_time_dynamic import DynamicActionBanditWithTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, datetime as dt\n",
    "\n",
    "# クラスのデフォルト：2022-01-01 の Unix 秒\n",
    "base_t = int(dt.datetime.timestamp(dt.datetime(2022, 1, 1)))\n",
    "one_week = 7 * 86400\n",
    "\n",
    "birth = np.array([base_t + i * one_week for i in range(10)], dtype=int)\n",
    "\n",
    "ds = DynamicActionBanditWithTime(\n",
    "    n_actions=10, dim_context=10,\n",
    "    action_birth_time=birth,\n",
    "    beta=0.1, random_state=123\n",
    ")\n",
    "\n",
    "train = ds.obtain_batch_bandit_feedback(n_rounds=20000)\n",
    "val   = ds.obtain_batch_bandit_feedback(\n",
    "    n_rounds=5000, evaluation_mode=True, time_at_evaluation=ds.t_future - 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_rounds', 'n_actions', 'context', 'time', 'action_context', 'action', 'position', 'reward', 'expected_reward', 'g_x_phi_t_a', 'h_x_t_a', 'pi_b', 'pscore', 'available_actions'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 Sony Group Corporation and Hanjuku-kaso Co., Ltd. All Rights Reserved.\n",
    "#\n",
    "# This software is released under the MIT License.\n",
    "\n",
    "\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from obp.dataset.base import BaseBanditDataset\n",
    "from obp.dataset.reward_type import RewardType\n",
    "from obp.types import BanditFeedback\n",
    "from obp.utils import check_array, sample_action_fast, softmax\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.utils import check_random_state, check_scalar\n",
    "\n",
    "SECONDS_PER_DAY = 24 * 60 * 60\n",
    "BIG_NUM = int(1e5)\n",
    "NUM_DAY_OF_WEEK = 7\n",
    "\n",
    "\n",
    "coef_func_signature = Callable[\n",
    "    [np.ndarray, np.ndarray, np.random.RandomState],\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "]\n",
    "\n",
    "\n",
    "def unix_time_to_season(unix_time):\n",
    "    # Convert Unix timestamp to a datetime object\n",
    "    datetime_converted = datetime.datetime.fromtimestamp(unix_time)\n",
    "    month = datetime_converted.month\n",
    "    # Get the season as an integer (0 = Spring, 1 = Summer, 2 = Fall, 3 = Winter)\n",
    "    if 1 <= month <= 3:\n",
    "        return 0\n",
    "    elif 4 <= month <= 6:\n",
    "        return 1\n",
    "    elif 7 <= month <= 9:\n",
    "        return 2\n",
    "    elif 10 <= month <= 12:\n",
    "        return 3\n",
    "\n",
    "\n",
    "def unix_time_to_month(unix_time):\n",
    "    # Convert Unix timestamp to a datetime object\n",
    "    datetime_converted = datetime.datetime.fromtimestamp(unix_time)\n",
    "    # Get the month as an integer (0 = Jan, 1 = Feb, ..., 11 = Dec)\n",
    "    month = datetime_converted.month\n",
    "    return month - 1\n",
    "\n",
    "\n",
    "def unix_time_to_day_of_week(unix_time):\n",
    "    # Convert Unix timestamp to a datetime object\n",
    "    datetime_converted = datetime.datetime.fromtimestamp(unix_time)\n",
    "    # Get the day of the week as an integer (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
    "    weekday = datetime_converted.weekday()\n",
    "    return weekday\n",
    "\n",
    "\n",
    "def unix_time_to_hour(unix_time):\n",
    "    # Convert Unix timestamp to a datetime object\n",
    "    datetime_converted = datetime.datetime.fromtimestamp(unix_time)\n",
    "    # Get the hour as an integer (0 = 0:00~0:59, 1 = 1:00~1:59, ..., 23 = 23:00~23:59)\n",
    "    hour = datetime_converted.hour\n",
    "    return hour\n",
    "\n",
    "\n",
    "def unix_time_to_AM_PM(unix_time):\n",
    "    # Convert Unix timestamp to a datetime object\n",
    "    datetime_converted = datetime.datetime.fromtimestamp(unix_time)\n",
    "    # Get the hour as an integer (0 = 0:00~0:59, 1 = 1:00~1:59, ..., 23 = 23:00~23:59)\n",
    "    hour = datetime_converted.hour\n",
    "    # Output 0 if the time is in AM, 1 if PM\n",
    "    if 0 <= hour < 12:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def unix_time_to_season_month(unix_time):\n",
    "    return unix_time_to_season(unix_time) * 12 + unix_time_to_month(unix_time)\n",
    "\n",
    "\n",
    "def unix_time_to_season_month_day_of_week(unix_time):\n",
    "    return (\n",
    "        unix_time_to_season(unix_time) * 84\n",
    "        + unix_time_to_month(unix_time) * 7\n",
    "        + unix_time_to_day_of_week(unix_time)\n",
    "    )\n",
    "\n",
    "\n",
    "def unix_time_to_season_month_day_of_week_AM_PM(unix_time):\n",
    "    return (\n",
    "        unix_time_to_season(unix_time) * 168\n",
    "        + unix_time_to_month(unix_time) * 14\n",
    "        + unix_time_to_day_of_week(unix_time) * 2\n",
    "        + unix_time_to_AM_PM(unix_time)\n",
    "    )\n",
    "\n",
    "\n",
    "def days_passed_to_time_structure_tree(days_passed, num_time_structure):\n",
    "    max_K_th_power_of_two = 1\n",
    "    for i in range(BIG_NUM):\n",
    "        max_K_th_power_of_two *= 2\n",
    "        if num_time_structure < max_K_th_power_of_two:\n",
    "            max_K_th_power_of_two /= 2\n",
    "            max_K_th_power_of_two = int(max_K_th_power_of_two)\n",
    "            break\n",
    "    remainder = int(num_time_structure % max_K_th_power_of_two)\n",
    "    if remainder == 0:\n",
    "        ref_days = 366 / max_K_th_power_of_two\n",
    "        ref_intervals = 366 / max_K_th_power_of_two\n",
    "        for i in range(num_time_structure):\n",
    "            if days_passed <= ref_days:\n",
    "                return i\n",
    "            ref_days += ref_intervals\n",
    "\n",
    "        return num_time_structure - 1\n",
    "    else:\n",
    "        ref_days = 366 / (max_K_th_power_of_two * 2)\n",
    "        ref_intervals = 366 / (max_K_th_power_of_two * 2)\n",
    "        for i in range(num_time_structure):\n",
    "            if days_passed <= ref_days:\n",
    "                return i\n",
    "            if i + 1 < 2 * remainder:\n",
    "                ref_days += ref_intervals\n",
    "            else:\n",
    "                ref_days += ref_intervals * 2\n",
    "\n",
    "        return num_time_structure - 1\n",
    "\n",
    "\n",
    "def unix_time_to_time_structure_n_tree(unix_time, num_time_structure):\n",
    "    # Convert the Unix timestamp to a datetime object\n",
    "    dt_object = datetime.datetime.fromtimestamp(unix_time)\n",
    "\n",
    "    # Get the year from the datetime object\n",
    "    year = dt_object.year\n",
    "\n",
    "    # Get the first day of the year for the given year\n",
    "    first_day_of_year = datetime.datetime(year, 1, 1)\n",
    "\n",
    "    # Calculate the number of days passed in the year\n",
    "    number_of_days_passed = (dt_object - first_day_of_year).days + 1\n",
    "\n",
    "    return days_passed_to_time_structure_tree(number_of_days_passed, num_time_structure)\n",
    "\n",
    "\n",
    "def obtain_num_time_structure(time_structure_func):\n",
    "    if time_structure_func == unix_time_to_season:\n",
    "        return 4\n",
    "    elif time_structure_func == unix_time_to_month:\n",
    "        return 12\n",
    "    elif time_structure_func == unix_time_to_day_of_week:\n",
    "        return 7\n",
    "    elif time_structure_func == unix_time_to_AM_PM:\n",
    "        return 2\n",
    "    elif time_structure_func == unix_time_to_hour:\n",
    "        return 24\n",
    "    elif time_structure_func == unix_time_to_season_month:\n",
    "        return 4 * 12\n",
    "    elif time_structure_func == unix_time_to_season_month_day_of_week:\n",
    "        return 4 * 12 * 7\n",
    "    elif time_structure_func == unix_time_to_season_month_day_of_week_AM_PM:\n",
    "        return 4 * 12 * 7 * 2\n",
    "\n",
    "\n",
    "def obtain_num_days_in_one_cycle(time_structure_func):\n",
    "    if time_structure_func == unix_time_to_season:\n",
    "        return 365\n",
    "    elif time_structure_func == unix_time_to_month:\n",
    "        return 365\n",
    "    elif time_structure_func == unix_time_to_day_of_week:\n",
    "        return 7\n",
    "    elif time_structure_func == unix_time_to_AM_PM:\n",
    "        return 1\n",
    "    elif time_structure_func == unix_time_to_hour:\n",
    "        return 1\n",
    "    elif time_structure_func == unix_time_to_season_month:\n",
    "        return 365\n",
    "    elif time_structure_func == unix_time_to_season_month_day_of_week:\n",
    "        return 365\n",
    "    elif time_structure_func == unix_time_to_season_month_day_of_week_AM_PM:\n",
    "        return 365\n",
    "    elif time_structure_func == unix_time_to_time_structure_n_tree:\n",
    "        return 365\n",
    "\n",
    "\n",
    "def sample_random_uniform_coefficients(\n",
    "    effective_dim_action_context: int,\n",
    "    effective_dim_context: int,\n",
    "    random_: np.random.RandomState,\n",
    "    **kwargs,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    context_coef_ = random_.uniform(-1, 1, size=effective_dim_context)\n",
    "    action_coef_ = random_.uniform(-1, 1, size=effective_dim_action_context)\n",
    "    context_action_coef_ = random_.uniform(\n",
    "        -1, 1, size=(effective_dim_context, effective_dim_action_context)\n",
    "    )\n",
    "    return context_coef_, action_coef_, context_action_coef_\n",
    "\n",
    "\n",
    "# Normalize time to [0, 1] (or [0, scale])\n",
    "def normalize_time(time, t_oldest, t_future, scale=1):\n",
    "    return scale * (time - t_oldest) / (t_future - t_oldest)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SyntheticBanditWithTimeDataset(BaseBanditDataset):\n",
    "    n_actions: int\n",
    "    dim_context: int = 1\n",
    "    n_users: int = None\n",
    "\n",
    "    # The oldest unix time when we can potentially observe logged bandit data\n",
    "    # ログデータの開始時間時間\n",
    "    t_oldest: int = int(\n",
    "        datetime.datetime.timestamp(datetime.datetime(year=2022, month=1, day=1))\n",
    "    )\n",
    "    # The latest unix time when we can potentially observe logged bandit data\n",
    "    # ログデータの終了時間時間\n",
    "    t_now: int = int(\n",
    "        datetime.datetime.timestamp(datetime.datetime(year=2022, month=6, day=1))\n",
    "    )\n",
    "    # The latest future unix time when we want to evaluate a target policy\n",
    "    # 評価データの上限\n",
    "    t_future: int = int(\n",
    "        datetime.datetime.timestamp(datetime.datetime(year=2023, month=1, day=1))\n",
    "    )\n",
    "\n",
    "    num_time_structure: int = 7\n",
    "\n",
    "    num_time_structure_for_context: int = 7\n",
    "\n",
    "    # q(x, t, a) = \\lambda * g(x, \\phi(t), a) + (1 - \\lambda) * h(x, t, a)\n",
    "    lambda_ratio: float = 0.95\n",
    "\n",
    "    # p(x|t) = \\alpha * p_1(x|\\phi_x(t)) + (1 - \\alpha) * p_2(x|t)\n",
    "    alpha_ratio: float = 0.95\n",
    "\n",
    "    reward_type: str = RewardType.CONTINUOUS.value\n",
    "\n",
    "    flag_simple_reward: bool = True\n",
    "\n",
    "    sample_non_stationary_context: bool = False\n",
    "\n",
    "    g_coef: int = 3\n",
    "    h_coef: int = 1\n",
    "\n",
    "    p_1_coef: int = 3\n",
    "    p_2_coef: int = 1\n",
    "\n",
    "    reward_function: Optional[Callable[[np.ndarray, np.ndarray], np.ndarray]] = None\n",
    "\n",
    "    reward_std: float = 1.0\n",
    "    action_context: Optional[np.ndarray] = None\n",
    "\n",
    "    behavior_policy_function: Optional[\n",
    "        Callable[[np.ndarray, np.ndarray], np.ndarray]\n",
    "    ] = None\n",
    "    beta: float = 1.0\n",
    "    n_deficient_actions: int = 0\n",
    "    random_state: int = 12345\n",
    "    dataset_name: str = \"synthetic_bandit_with_time_dataset\"\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize Class.\"\"\"\n",
    "        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n",
    "        check_scalar(self.dim_context, \"dim_context\", int, min_val=1)\n",
    "        check_scalar(self.beta, \"beta\", (int, float))\n",
    "        check_scalar(\n",
    "            self.n_deficient_actions,\n",
    "            \"n_deficient_actions\",\n",
    "            int,\n",
    "            min_val=0,\n",
    "            max_val=self.n_actions - 1,\n",
    "        )\n",
    "\n",
    "        if self.random_state is None:\n",
    "            raise ValueError(\"`random_state` must be given\")\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "\n",
    "        if RewardType(self.reward_type) not in [\n",
    "            RewardType.BINARY,\n",
    "            RewardType.CONTINUOUS,\n",
    "        ]:\n",
    "            raise ValueError(\n",
    "                f\"`reward_type` must be either '{RewardType.BINARY.value}' or '{RewardType.CONTINUOUS.value}',\"\n",
    "                f\"but {self.reward_type} is given.'\"\n",
    "            )\n",
    "        check_scalar(self.reward_std, \"reward_std\", (int, float), min_val=0)\n",
    "        if self.reward_function is None:\n",
    "            self.expected_reward = self.sample_contextfree_expected_reward()\n",
    "        if RewardType(self.reward_type) == RewardType.CONTINUOUS:\n",
    "            self.reward_min = 0\n",
    "            self.reward_max = 1e10\n",
    "\n",
    "        # one-hot encoding characterizing actions.\n",
    "        if self.action_context is None:\n",
    "            self.action_context = np.eye(self.n_actions, dtype=int)\n",
    "        else:\n",
    "            check_array(\n",
    "                array=self.action_context, name=\"action_context\", expected_dim=2\n",
    "            )\n",
    "            if self.action_context.shape[0] != self.n_actions:\n",
    "                raise ValueError(\n",
    "                    \"Expected `action_context.shape[0] == n_actions`, but found it False.\"\n",
    "                )\n",
    "        self._define_param_for_q_and_h()\n",
    "\n",
    "        def true_time_structure_func_for_reward(unix_time):\n",
    "            return unix_time_to_time_structure_n_tree(\n",
    "                unix_time, self.num_time_structure\n",
    "            )\n",
    "\n",
    "        self.time_structure_func = true_time_structure_func_for_reward\n",
    "\n",
    "        def true_time_structure_func_for_context(unix_time):\n",
    "            return unix_time_to_time_structure_n_tree(\n",
    "                unix_time, self.num_time_structure_for_context\n",
    "            )\n",
    "\n",
    "        self.time_structure_func_for_context = true_time_structure_func_for_context\n",
    "\n",
    "    # Set the parameters used for construting g(x, \\phi(t), a) and h(x, t, a)\n",
    "    def _define_param_for_q_and_h(self) -> None:\n",
    "        # Sample parameters from Unif([-h_coef, h_coef]) for generating h(x, t, a)\n",
    "        self.theta_x = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=self.dim_context\n",
    "        )\n",
    "        self.theta_t = self.random_.uniform(low=-self.h_coef, high=self.h_coef, size=2)\n",
    "        self.theta_a = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=self.n_actions\n",
    "        )\n",
    "        self.theta_t_a_1 = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=self.n_actions\n",
    "        )\n",
    "        self.theta_t_a_2 = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=3\n",
    "        )\n",
    "        self.N_x_a = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=(self.dim_context, self.n_actions)\n",
    "        )\n",
    "        self.theta_finer_phi_t = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=NUM_DAY_OF_WEEK\n",
    "        )\n",
    "        self.N_finer_phi_t_a_1 = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=(NUM_DAY_OF_WEEK, self.n_actions)\n",
    "        )\n",
    "        self.P_x_finer_phi_t_a = self.random_.uniform(\n",
    "            low=-self.h_coef,\n",
    "            high=self.h_coef,\n",
    "            size=(self.dim_context, NUM_DAY_OF_WEEK, self.n_actions),\n",
    "        )\n",
    "\n",
    "        # Sample parameters from Unif([-g_coef, g_coef]) for generating g(x, \\phi(t), a)\n",
    "        self.psi_x = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=self.dim_context\n",
    "        )\n",
    "        self.psi_phi_t = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=self.num_time_structure\n",
    "        )\n",
    "        self.psi_a = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=self.n_actions\n",
    "        )\n",
    "        self.M_phi_t_a = self.random_.uniform(\n",
    "            low=-self.g_coef,\n",
    "            high=self.g_coef,\n",
    "            size=(self.num_time_structure, self.n_actions),\n",
    "        )\n",
    "        self.M_x_a = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=(self.dim_context, self.n_actions)\n",
    "        )\n",
    "        self.P_x_phi_t_a = self.random_.uniform(\n",
    "            low=-self.g_coef,\n",
    "            high=self.g_coef,\n",
    "            size=(self.dim_context, self.num_time_structure, self.n_actions),\n",
    "        )\n",
    "\n",
    "        # Sample parameters from Unif([-p_1_corf, p_1_coef]) for generating p_1(x|t)\n",
    "        self.gamma = self.random_.uniform(\n",
    "            low=-self.p_1_coef,\n",
    "            high=self.p_1_coef,\n",
    "            size=self.num_time_structure_for_context,\n",
    "        )\n",
    "\n",
    "        # Sample parameters from Unif([-p_2_corf, p_2_coef]) for generating p_1(x|t)\n",
    "        self.delta = self.random_.uniform(\n",
    "            low=-self.p_2_coef, high=self.p_2_coef, size=5\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def len_list(self) -> int:\n",
    "        \"\"\"Length of recommendation lists, slate size.\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def sample_contextfree_expected_reward(self) -> np.ndarray:\n",
    "        \"\"\"Sample expected reward for each action from the uniform distribution.\"\"\"\n",
    "        return self.random_.uniform(size=self.n_actions)\n",
    "\n",
    "    def calc_expected_reward(self, context: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sample expected rewards given contexts\"\"\"\n",
    "        # sample reward for each round based on the reward function\n",
    "        if self.reward_function is None:\n",
    "            expected_reward_ = np.tile(self.expected_reward, (context.shape[0], 1))\n",
    "        else:\n",
    "            expected_reward_ = self.reward_function(\n",
    "                context=context,\n",
    "                action_context=self.action_context,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "\n",
    "        return expected_reward_\n",
    "\n",
    "    def sample_reward_given_expected_reward(\n",
    "        self,\n",
    "        expected_reward: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Sample reward given expected rewards\"\"\"\n",
    "        expected_reward_factual = expected_reward[np.arange(action.shape[0]), action]\n",
    "        if RewardType(self.reward_type) == RewardType.BINARY:\n",
    "            reward = self.random_.binomial(n=1, p=expected_reward_factual)\n",
    "        elif RewardType(self.reward_type) == RewardType.CONTINUOUS:\n",
    "            mean = expected_reward_factual\n",
    "            a = (self.reward_min - mean) / self.reward_std\n",
    "            b = (self.reward_max - mean) / self.reward_std\n",
    "            reward = truncnorm.rvs(\n",
    "                a=a,\n",
    "                b=b,\n",
    "                loc=mean,\n",
    "                scale=self.reward_std,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def sample_reward(self, context: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        check_array(array=context, name=\"context\", expected_dim=2)\n",
    "        check_array(array=action, name=\"action\", expected_dim=1)\n",
    "        if context.shape[0] != action.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Expected `context.shape[0] == action.shape[0]`, but found it False\"\n",
    "            )\n",
    "        if not np.issubdtype(action.dtype, np.integer):\n",
    "            raise ValueError(\"the dtype of action must be a subdtype of int\")\n",
    "\n",
    "        expected_reward_ = self.calc_expected_reward(context)\n",
    "\n",
    "        return self.sample_reward_given_expected_reward(expected_reward_, action)\n",
    "\n",
    "    def synthesize_expected_reward(self, contexts, times):\n",
    "        n_rounds = contexts.shape[0]\n",
    "\n",
    "        # Convert Unix timestamp to a datetime object\n",
    "        finer_time_structure_func = np.vectorize(datetime.datetime.utcfromtimestamp)\n",
    "        dt_objects = finer_time_structure_func(times)\n",
    "\n",
    "        # Assuming dt_objects is a NumPy array of datetime objects\n",
    "        get_day_of_week = np.vectorize(lambda dt: dt.weekday())\n",
    "        days_of_week = get_day_of_week(dt_objects)\n",
    "\n",
    "        finer_time_structure_context = np.zeros(shape=(n_rounds, NUM_DAY_OF_WEEK))\n",
    "\n",
    "        row_indices = np.arange(n_rounds)\n",
    "        column_indices = days_of_week\n",
    "\n",
    "        finer_time_structure_context[row_indices, column_indices] = 1\n",
    "\n",
    "        time_structure_func_vec = np.vectorize(self.time_structure_func)\n",
    "\n",
    "        time_structures = time_structure_func_vec(times)\n",
    "\n",
    "        time_structure_context = np.zeros(shape=(n_rounds, self.num_time_structure))\n",
    "\n",
    "        row_indices = np.arange(n_rounds)\n",
    "        column_indices = time_structures\n",
    "\n",
    "        time_structure_context[row_indices, column_indices] = 1\n",
    "\n",
    "        # Synthetize h(x, t, a)\n",
    "        # if h(x, t, a) is a simple or comlex function\n",
    "\n",
    "        # Initialize h(x, t, a) by zero matrix\n",
    "        h_x_t_a_ = np.zeros((n_rounds, self.n_actions))\n",
    "\n",
    "        # Synthesize each of the componets to synthesize h(x, t, a)\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            h_1_x = (contexts[:, 0:6].sum(axis=1) < 2.5) * self.theta_x[0]\n",
    "            h_1_x += (contexts[:, 7:9].sum(axis=1) < -0.5) * self.theta_x[1]\n",
    "            h_1_x += (contexts[:, 2:5].sum(axis=1) > 2.0) * self.theta_x[2]\n",
    "        else:\n",
    "            h_1_x = contexts @ self.theta_x / self.dim_context\n",
    "\n",
    "        h_2_t = finer_time_structure_context @ self.theta_finer_phi_t\n",
    "\n",
    "        h_3_a = self.action_context @ self.theta_a\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            shrinked_contexts = np.concatenate(\n",
    "                [\n",
    "                    (contexts[:, 0:4].sum(axis=1) < 3).reshape(-1, 1),\n",
    "                    (contexts[:, 2:9].sum(axis=1) > 2.5).reshape(-1, 1),\n",
    "                    (contexts[:, 1:7].sum(axis=1) < 1.5).reshape(-1, 1),\n",
    "                    (contexts[:, 6:10].sum(axis=1) > -1.5).reshape(-1, 1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            h_5_x_a = shrinked_contexts @ self.N_x_a[0:4, :] @ self.action_context\n",
    "        else:\n",
    "            h_5_x_a = contexts @ self.N_x_a @ self.action_context / self.dim_context\n",
    "\n",
    "        h_6_t_a = (\n",
    "            finer_time_structure_context @ self.N_finer_phi_t_a_1 @ self.action_context\n",
    "        )\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            shrinked_contexts = np.concatenate(\n",
    "                [\n",
    "                    (contexts[:, 0:4].sum(axis=1) < 4).reshape(-1, 1),\n",
    "                    (contexts[:, 2:9].sum(axis=1) > 3.5).reshape(-1, 1),\n",
    "                    (contexts[:, 2:5].sum(axis=1) > 1.5).reshape(-1, 1),\n",
    "                    (contexts[:, 5:10].sum(axis=1) < -2.5).reshape(-1, 1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            h_7_x_phi_t_a = np.einsum(\n",
    "                \"ij,jkl->ikl\", shrinked_contexts, self.P_x_finer_phi_t_a[0:4, :, :]\n",
    "            )\n",
    "            h_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", h_7_x_phi_t_a, finer_time_structure_context\n",
    "            )\n",
    "            h_7_x_phi_t_a = h_7_x_phi_t_a @ self.action_context\n",
    "        else:\n",
    "            h_7_x_phi_t_a = (\n",
    "                np.einsum(\"ij,jkl->ikl\", contexts, self.P_x_finer_phi_t_a)\n",
    "                / self.dim_context\n",
    "            )\n",
    "            h_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", h_7_x_phi_t_a, finer_time_structure_context\n",
    "            )\n",
    "            h_7_x_phi_t_a = h_7_x_phi_t_a @ self.action_context\n",
    "\n",
    "        h_x_t_a_ = (\n",
    "            h_1_x[:, np.newaxis]\n",
    "            + h_2_t[:, np.newaxis]\n",
    "            + h_3_a\n",
    "            + h_5_x_a\n",
    "            + h_6_t_a\n",
    "            + h_7_x_phi_t_a\n",
    "        )\n",
    "\n",
    "        # Synthetize g(x, \\phi(t), a)\n",
    "        g_x_phi_t_a_ = np.zeros((n_rounds, self.n_actions))\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            g_1_x = (contexts[:, 0:4].sum(axis=1) < 1.5) * self.psi_x[0]\n",
    "            g_1_x += (contexts[:, 5:9].sum(axis=1) < -0.5) * self.psi_x[1]\n",
    "            g_1_x += (contexts[:, 3:5].sum(axis=1) > 3.0) * self.psi_x[2]\n",
    "            g_1_x += (contexts[:, 6:10].sum(axis=1) < 1.0) * self.psi_x[3]\n",
    "        else:\n",
    "            g_1_x = contexts @ self.psi_x / self.dim_context\n",
    "\n",
    "        g_2_phi_t = time_structure_context @ self.psi_phi_t\n",
    "\n",
    "        g_6_phi_t_a = time_structure_context @ self.M_phi_t_a @ self.action_context\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            shrinked_contexts = np.concatenate(\n",
    "                [\n",
    "                    (contexts[:, 0:4].sum(axis=1) < 4).reshape(-1, 1),\n",
    "                    (contexts[:, 5:9].sum(axis=1) > 3).reshape(-1, 1),\n",
    "                    (contexts[:, 2:10].sum(axis=1) < -2.5).reshape(-1, 1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            g_7_x_phi_t_a = np.einsum(\n",
    "                \"ij,jkl->ikl\", shrinked_contexts, self.P_x_phi_t_a[0:3, :, :]\n",
    "            )\n",
    "            g_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", g_7_x_phi_t_a, time_structure_context\n",
    "            )\n",
    "            g_7_x_phi_t_a = g_7_x_phi_t_a @ self.action_context\n",
    "        else:\n",
    "            g_7_x_phi_t_a = (\n",
    "                np.einsum(\"ij,jkl->ikl\", contexts, self.P_x_phi_t_a) / self.dim_context\n",
    "            )\n",
    "            g_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", g_7_x_phi_t_a, time_structure_context\n",
    "            )\n",
    "            g_7_x_phi_t_a = g_7_x_phi_t_a @ self.action_context\n",
    "\n",
    "        # Take the sum of each vector or matrices to consturct h(x, t, a)\n",
    "        g_x_phi_t_a_ = (\n",
    "            g_1_x[:, np.newaxis]\n",
    "            + g_2_phi_t[:, np.newaxis]\n",
    "            + g_6_phi_t_a\n",
    "            + g_7_x_phi_t_a\n",
    "        )\n",
    "\n",
    "        # q(x, t, a) = \\lambda * g(x, \\phi(t), a) + (1 - \\lambda) * h(x, t, a)\n",
    "        expected_reward_ = (\n",
    "            self.lambda_ratio * g_x_phi_t_a_ + (1 - self.lambda_ratio) * h_x_t_a_\n",
    "        )\n",
    "\n",
    "        return g_x_phi_t_a_, h_x_t_a_, expected_reward_\n",
    "\n",
    "    def obtain_batch_bandit_feedback(\n",
    "        self,\n",
    "        n_rounds: int,\n",
    "        evaluation_mode=False,\n",
    "        time_at_evaluation=0,\n",
    "        random_state_for_sampling=None,\n",
    "    ) -> BanditFeedback:\n",
    "        check_scalar(n_rounds, \"n_rounds\", int, min_val=1)\n",
    "\n",
    "        random_for_sample_ = check_random_state(\n",
    "            random_state_for_sampling + self.random_state\n",
    "        )\n",
    "\n",
    "        # Observe time\n",
    "        if evaluation_mode == False:\n",
    "            # Sample time data with size n from the uniform distribution ranging from t_oldest to t_now\n",
    "            times = random_for_sample_.uniform(\n",
    "                self.t_oldest, self.t_now, size=n_rounds\n",
    "            ).astype(int)\n",
    "\n",
    "            times.sort()\n",
    "        else:\n",
    "            # All time are time_at_evaluation\n",
    "            times = np.full(n_rounds, time_at_evaluation)\n",
    "\n",
    "        # Observe context\n",
    "        # Stationary context\n",
    "        if self.sample_non_stationary_context == False:\n",
    "            contexts = random_for_sample_.normal(size=(n_rounds, self.dim_context))\n",
    "\n",
    "        # Non-stationary context\n",
    "        else:\n",
    "            # normalize the time vector\n",
    "            normalized_time = normalize_time(times, self.t_oldest, self.t_future)\n",
    "\n",
    "            time_structure_func_for_context_vec = np.vectorize(\n",
    "                self.time_structure_func_for_context\n",
    "            )\n",
    "\n",
    "            time_structures_for_context = time_structure_func_for_context_vec(times)\n",
    "\n",
    "            time_structure_context_for_context = np.zeros(\n",
    "                shape=(n_rounds, self.num_time_structure_for_context)\n",
    "            )\n",
    "\n",
    "            row_indices = np.arange(n_rounds)\n",
    "            column_indices = time_structures_for_context\n",
    "\n",
    "            time_structure_context_for_context[row_indices, column_indices] = 1\n",
    "\n",
    "            mu_1 = time_structure_context_for_context @ self.gamma\n",
    "\n",
    "            Sigma_1 = 1\n",
    "\n",
    "            mu_2 = self.delta[0] * normalized_time\n",
    "\n",
    "            Sigma_2 = 1\n",
    "\n",
    "            # Augment the mean vector to the matrix\n",
    "            mu_1_mat = mu_1[:, np.newaxis]\n",
    "            mu_1_mat = mu_1_mat * np.ones((1, self.dim_context))\n",
    "\n",
    "            mu_2_mat = mu_2[:, np.newaxis]\n",
    "            mu_2_mat = mu_2_mat * np.ones((1, self.dim_context))\n",
    "\n",
    "            # Sample each of the elements to construct the context\n",
    "            contexts_1 = random_for_sample_.normal(\n",
    "                size=(n_rounds, self.dim_context), loc=mu_1_mat, scale=Sigma_1\n",
    "            )\n",
    "            contexts_2 = random_for_sample_.normal(\n",
    "                size=(n_rounds, self.dim_context), loc=mu_2_mat, scale=Sigma_2\n",
    "            )\n",
    "            # Synthetize the context\n",
    "            contexts = (\n",
    "                self.alpha_ratio * contexts_1 + (1 - self.alpha_ratio) * contexts_2\n",
    "            )\n",
    "\n",
    "        g_x_phi_t_a_, h_x_t_a_, expected_reward_ = self.synthesize_expected_reward(\n",
    "            contexts, times\n",
    "        )\n",
    "\n",
    "        if RewardType(self.reward_type) == RewardType.CONTINUOUS:\n",
    "            # correct expected_reward_, as we use truncated normal distribution here\n",
    "            mean = expected_reward_\n",
    "            a = (self.reward_min - mean) / self.reward_std\n",
    "            b = (self.reward_max - mean) / self.reward_std\n",
    "            expected_reward_ = truncnorm.stats(\n",
    "                a=a, b=b, loc=mean, scale=self.reward_std, moments=\"m\"\n",
    "            )\n",
    "\n",
    "        # calculate the action choice probabilities of the behavior policy\n",
    "        if self.behavior_policy_function is None:\n",
    "            pi_b_logits = expected_reward_\n",
    "        else:\n",
    "            pi_b_logits = self.behavior_policy_function(\n",
    "                context=contexts,\n",
    "                action_context=self.action_context,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        # create some deficient actions based on the value of `n_deficient_actions`\n",
    "        if self.n_deficient_actions > 0:\n",
    "            pi_b = np.zeros_like(pi_b_logits)\n",
    "            n_supported_actions = self.n_actions - self.n_deficient_actions\n",
    "            supported_actions = np.argsort(\n",
    "                self.random_.gumbel(size=(n_rounds, self.n_actions)), axis=1\n",
    "            )[:, ::-1][:, :n_supported_actions]\n",
    "            supported_actions_idx = (\n",
    "                np.tile(np.arange(n_rounds), (n_supported_actions, 1)).T,\n",
    "                supported_actions,\n",
    "            )\n",
    "            pi_b[supported_actions_idx] = softmax(\n",
    "                self.beta * pi_b_logits[supported_actions_idx]\n",
    "            )\n",
    "        else:\n",
    "            pi_b = softmax(self.beta * pi_b_logits)\n",
    "        # sample actions for each round based on the behavior policy\n",
    "        actions = sample_action_fast(pi_b, random_state=self.random_state)\n",
    "\n",
    "        # sample rewards based on the context and action\n",
    "        rewards = self.sample_reward_given_expected_reward(expected_reward_, actions)\n",
    "\n",
    "        return dict(\n",
    "            n_rounds=n_rounds,\n",
    "            n_actions=self.n_actions,\n",
    "            context=contexts,\n",
    "            time=times,\n",
    "            action_context=self.action_context,\n",
    "            action=actions,\n",
    "            position=None,\n",
    "            reward=rewards,\n",
    "            expected_reward=expected_reward_,\n",
    "            g_x_phi_t_a=g_x_phi_t_a_,\n",
    "            h_x_t_a=h_x_t_a_,\n",
    "            pi_b=pi_b[:, :, np.newaxis],\n",
    "            pscore=pi_b[np.arange(n_rounds), actions],\n",
    "        )\n",
    "\n",
    "    def calc_ground_truth_policy_value(\n",
    "        self, expected_reward: np.ndarray, action_dist: np.ndarray\n",
    "    ) -> float:\n",
    "        return np.average(expected_reward, weights=action_dist, axis=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_overlaps = 1\n",
    "t_now = int(\n",
    "    datetime.datetime.timestamp(\n",
    "        datetime.datetime(year=2022, month=12, day=31, hour=23, minute=59, second=59)\n",
    "    )\n",
    ")\n",
    "t_oldest = int(\n",
    "    datetime.datetime.timestamp(\n",
    "        datetime.datetime(\n",
    "            year=2023 - num_overlaps, month=1, day=1, hour=0, minute=0, second=0\n",
    "        )\n",
    "    )\n",
    ")\n",
    "time_at_evaluation = int(\n",
    "    datetime.datetime.timestamp(\n",
    "        datetime.datetime(year=2023, month=1, day=1, hour=0, minute=0, second=0)\n",
    "    )\n",
    ")\n",
    "t_future = int(\n",
    "    datetime.datetime.timestamp(\n",
    "        datetime.datetime(year=2024, month=1, day=1, hour=0, minute=0, second=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = SyntheticBanditWithTimeDataset(\n",
    "        n_actions=1000, \n",
    "        dim_context=10,\n",
    "        n_users=1000, \n",
    "        t_oldest = t_oldest,\n",
    "        t_now = t_now,\n",
    "        t_future = t_future,\n",
    "        beta = 0.2, \n",
    "        reward_std = 1, \n",
    "        num_time_structure=8, \n",
    "        # lambda_ratio = conf.lambda_ratio, \n",
    "        # flag_simple_reward = conf.flag_simple_reward, \n",
    "        # g_coef=conf.g_coef, \n",
    "        # h_coef=conf.h_coef, \n",
    "        # random_state=conf.random_state + h * 10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyntheticBanditWithTimeDataset(n_actions=1000, dim_context=10, n_users=1000, t_oldest=1640962800, t_now=1672498799, t_future=1704034800, num_time_structure=8, num_time_structure_for_context=7, lambda_ratio=0.95, alpha_ratio=0.95, reward_type='continuous', flag_simple_reward=True, sample_non_stationary_context=False, g_coef=3, h_coef=1, p_1_coef=3, p_2_coef=1, reward_function=None, reward_std=1, action_context=array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]]), behavior_policy_function=None, beta=0.2, n_deficient_actions=0, random_state=12345, dataset_name='synthetic_bandit_with_time_dataset')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SyntheticBanditWithTimeDataset(BaseBanditDataset):\n",
    "    n_actions: int\n",
    "    dim_context: int = 1\n",
    "    n_users: int = None\n",
    "\n",
    "    # The oldest unix time when we can potentially observe logged bandit data\n",
    "    # ログデータの開始時間時間\n",
    "    t_oldest: int = int(\n",
    "        datetime.datetime.timestamp(datetime.datetime(year=2022, month=1, day=1))\n",
    "    )\n",
    "    # The latest unix time when we can potentially observe logged bandit data\n",
    "    # ログデータの終了時間時間\n",
    "    t_now: int = int(\n",
    "        datetime.datetime.timestamp(datetime.datetime(year=2022, month=6, day=1))\n",
    "    )\n",
    "\n",
    "    # The latest future unix time when we want to evaluate a target policy\n",
    "    # 評価データの上限\n",
    "    t_future: int = int(\n",
    "        datetime.datetime.timestamp(datetime.datetime(year=2023, month=1, day=1))\n",
    "    )\n",
    "\n",
    "    num_time_structure: int = 7\n",
    "\n",
    "    num_time_structure_for_context: int = 7\n",
    "\n",
    "    # q(x, t, a) = \\lambda * g(x, \\phi(t), a) + (1 - \\lambda) * h(x, t, a)\n",
    "    lambda_ratio: float = 0.95\n",
    "\n",
    "    # p(x|t) = \\alpha * p_1(x|\\phi_x(t)) + (1 - \\alpha) * p_2(x|t)\n",
    "    alpha_ratio: float = 0.95\n",
    "\n",
    "    reward_type: str = RewardType.CONTINUOUS.value\n",
    "\n",
    "    flag_simple_reward: bool = True\n",
    "\n",
    "    sample_non_stationary_context: bool = False\n",
    "\n",
    "    g_coef: int = 3\n",
    "    h_coef: int = 1\n",
    "\n",
    "    p_1_coef: int = 3\n",
    "    p_2_coef: int = 1\n",
    "\n",
    "    reward_function: Optional[Callable[[np.ndarray, np.ndarray], np.ndarray]] = None\n",
    "\n",
    "    reward_std: float = 1.0\n",
    "    action_context: Optional[np.ndarray] = None\n",
    "\n",
    "    behavior_policy_function: Optional[\n",
    "        Callable[[np.ndarray, np.ndarray], np.ndarray]\n",
    "    ] = None\n",
    "    beta: float = 1.0\n",
    "    n_deficient_actions: int = 0\n",
    "    random_state: int = 12345\n",
    "    dataset_name: str = \"synthetic_bandit_with_time_dataset\"\n",
    "\n",
    "    #行動特徴量ように追加\n",
    "    n_cat_per_dim: int = 10\n",
    "    latent_param_mat_dim: int = 5\n",
    "    n_cat_dim: int = 3\n",
    "    p_e_a_param_std: Union[int, float] = 1.0\n",
    "    n_unobserved_cat_dim: int = 0\n",
    "    n_irrelevant_cat_dim: int = 0\n",
    "    num_time_structure_for_action: int = 7\n",
    "    l_1_coef: int = 3\n",
    "    l_2_coef: int = 1\n",
    "    # z_e(t) = \\delta * l_1(\\phi(t), a) + (1 - \\delta) * l_2(t, a)\n",
    "    delta_ratio: float = 0.95\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Initialize Class.\"\"\"\n",
    "        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n",
    "        check_scalar(self.dim_context, \"dim_context\", int, min_val=1)\n",
    "        check_scalar(self.beta, \"beta\", (int, float))\n",
    "        # 追加の処理\n",
    "        check_scalar(self.n_cat_per_dim, \"n_cat_per_dim\", int, min_val=1)\n",
    "        check_scalar(self.latent_param_mat_dim, \"latent_param_mat_dim\", int, min_val=1)\n",
    "        check_scalar(self.n_cat_dim, \"n_cat_dim\", int, min_val=1)\n",
    "        check_scalar(self.p_e_a_param_std, \"p_e_a_param_std\", (int, float), min_val=0.0)\n",
    "\n",
    "        # check_scalar(\n",
    "        #     self.n_deficient_actions,\n",
    "        #     \"n_deficient_actions\",\n",
    "        #     int,\n",
    "        #     min_val=0,\n",
    "        #     max_val=self.n_actions - 1,\n",
    "        # )\n",
    "        check_scalar(\n",
    "            self.n_unobserved_cat_dim,\n",
    "            \"n_unobserved_cat_dim\",\n",
    "            int,\n",
    "            min_val=0,\n",
    "            max_val=self.n_cat_dim,\n",
    "        )\n",
    "        check_scalar(\n",
    "            self.n_irrelevant_cat_dim,\n",
    "            \"n_irrelevant_cat_dim\",\n",
    "            int,\n",
    "            min_val=0,\n",
    "            max_val=self.n_cat_dim,\n",
    "        )\n",
    "        self.n_cat_dim += 1\n",
    "        self.n_unobserved_cat_dim += 1\n",
    "        self.n_irrelevant_cat_dim += 1\n",
    "        self._define_action_embed_with_time()\n",
    "\n",
    "        if self.random_state is None:\n",
    "            raise ValueError(\"`random_state` must be given\")\n",
    "        self.random_ = check_random_state(self.random_state)\n",
    "\n",
    "        if RewardType(self.reward_type) not in [\n",
    "            RewardType.BINARY,\n",
    "            RewardType.CONTINUOUS,\n",
    "        ]:\n",
    "            raise ValueError(\n",
    "                f\"`reward_type` must be either '{RewardType.BINARY.value}' or '{RewardType.CONTINUOUS.value}',\"\n",
    "                f\"but {self.reward_type} is given.'\"\n",
    "            )\n",
    "        check_scalar(self.reward_std, \"reward_std\", (int, float), min_val=0)\n",
    "        if self.reward_function is None:\n",
    "            self.expected_reward = self.sample_contextfree_expected_reward()\n",
    "        if RewardType(self.reward_type) == RewardType.CONTINUOUS:\n",
    "            self.reward_min = 0\n",
    "            self.reward_max = 1e10\n",
    "\n",
    "        # one-hot encoding characterizing actions.\n",
    "        if self.action_context is None:\n",
    "            self.action_context = np.eye(self.n_actions, dtype=int)\n",
    "        else:\n",
    "            check_array(\n",
    "                array=self.action_context, name=\"action_context\", expected_dim=2\n",
    "            )\n",
    "            if self.action_context.shape[0] != self.n_actions:\n",
    "                raise ValueError(\n",
    "                    \"Expected `action_context.shape[0] == n_actions`, but found it False.\"\n",
    "                )\n",
    "        self._define_param_for_q_and_h()\n",
    "\n",
    "        def true_time_structure_func_for_reward(unix_time):\n",
    "            return unix_time_to_time_structure_n_tree(\n",
    "                unix_time, self.num_time_structure\n",
    "            )\n",
    "\n",
    "        self.time_structure_func = true_time_structure_func_for_reward\n",
    "\n",
    "        def true_time_structure_func_for_context(unix_time):\n",
    "            return unix_time_to_time_structure_n_tree(\n",
    "                unix_time, self.num_time_structure_for_context\n",
    "            )\n",
    "\n",
    "        self.time_structure_func_for_context = true_time_structure_func_for_context\n",
    "\n",
    "        def true_time_structure_func_for_action(unix_time):\n",
    "            return unix_time_to_time_structure_n_tree(\n",
    "                unix_time, self.num_time_structure_for_action\n",
    "            )\n",
    "\n",
    "        self.time_structure_func_for_action = true_time_structure_func_for_action\n",
    "\n",
    "\n",
    "    # Set the parameters used for construting g(x, \\phi(t), a) and h(x, t, a)\n",
    "    def _define_param_for_q_and_h(self) -> None:\n",
    "        # Sample parameters from Unif([-h_coef, h_coef]) for generating h(x, t, a)\n",
    "        self.theta_x = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=self.dim_context\n",
    "        )\n",
    "        self.theta_t = self.random_.uniform(low=-self.h_coef, high=self.h_coef, size=2)\n",
    "        self.theta_a = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=self.n_actions\n",
    "        )\n",
    "        self.theta_t_a_1 = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=self.n_actions\n",
    "        )\n",
    "        self.theta_t_a_2 = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=3\n",
    "        )\n",
    "        self.N_x_a = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=(self.dim_context, self.n_actions)\n",
    "        )\n",
    "        self.theta_finer_phi_t = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=NUM_DAY_OF_WEEK\n",
    "        )\n",
    "        self.N_finer_phi_t_a_1 = self.random_.uniform(\n",
    "            low=-self.h_coef, high=self.h_coef, size=(NUM_DAY_OF_WEEK, self.n_actions)\n",
    "        )\n",
    "        self.P_x_finer_phi_t_a = self.random_.uniform(\n",
    "            low=-self.h_coef,\n",
    "            high=self.h_coef,\n",
    "            size=(self.dim_context, NUM_DAY_OF_WEEK, self.n_actions),\n",
    "        )\n",
    "\n",
    "        # Sample parameters from Unif([-g_coef, g_coef]) for generating g(x, \\phi(t), a)\n",
    "        self.psi_x = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=self.dim_context\n",
    "        )\n",
    "        self.psi_phi_t = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=self.num_time_structure\n",
    "        )\n",
    "        self.psi_a = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=self.n_actions\n",
    "        )\n",
    "        self.M_phi_t_a = self.random_.uniform(\n",
    "            low=-self.g_coef,\n",
    "            high=self.g_coef,\n",
    "            size=(self.num_time_structure, self.n_actions),\n",
    "        )\n",
    "        self.M_x_a = self.random_.uniform(\n",
    "            low=-self.g_coef, high=self.g_coef, size=(self.dim_context, self.n_actions)\n",
    "        )\n",
    "        self.P_x_phi_t_a = self.random_.uniform(\n",
    "            low=-self.g_coef,\n",
    "            high=self.g_coef,\n",
    "            size=(self.dim_context, self.num_time_structure, self.n_actions),\n",
    "        )\n",
    "\n",
    "        # Sample parameters from Unif([-p_1_corf, p_1_coef]) for generating p_1(x|t)\n",
    "        self.gamma = self.random_.uniform(\n",
    "            low=-self.p_1_coef,\n",
    "            high=self.p_1_coef,\n",
    "            size=self.num_time_structure_for_context,\n",
    "        )\n",
    "\n",
    "        # Sample parameters from Unif([-p_2_corf, p_2_coef]) for generating p_1(x|t)\n",
    "        self.delta = self.random_.uniform(\n",
    "            low=-self.p_2_coef, high=self.p_2_coef, size=5\n",
    "        )\n",
    "    \n",
    "    ## 追加\n",
    "    def _define_action_embed_with_time(self) -> None:\n",
    "        \"\"\"時間依存の行動埋め込みパラメータを定義する\"\"\"\n",
    "\n",
    "        # ベースの潜在表現（不変部分）\n",
    "        self.latent_cat_param_base = self.random_.normal(\n",
    "            size=(self.n_cat_dim, self.n_cat_per_dim, self.latent_param_mat_dim)\n",
    "        )\n",
    "\n",
    "        # φ(t) に依存する周期的パラメータ\n",
    "        self.latent_cat_param_phi = self.random_.normal(\n",
    "            size=(self.num_time_structure_for_action,\n",
    "                self.n_cat_dim,\n",
    "                self.n_cat_per_dim,\n",
    "                self.latent_param_mat_dim)\n",
    "        )\n",
    "\n",
    "        # t（連続時間）に依存するトレンドパラメータ\n",
    "        self.latent_cat_param_t = self.random_.normal(\n",
    "            size=(self.n_actions,\n",
    "                self.n_cat_dim,\n",
    "                self.n_cat_per_dim,\n",
    "                self.latent_param_mat_dim)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def len_list(self) -> int:\n",
    "        \"\"\"Length of recommendation lists, slate size.\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def sample_contextfree_expected_reward(self) -> np.ndarray:\n",
    "        \"\"\"Sample expected reward for each action from the uniform distribution.\"\"\"\n",
    "        return self.random_.uniform(size=self.n_actions)\n",
    "\n",
    "    def calc_expected_reward(self, context: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sample expected rewards given contexts\"\"\"\n",
    "        # sample reward for each round based on the reward function\n",
    "        if self.reward_function is None:\n",
    "            expected_reward_ = np.tile(self.expected_reward, (context.shape[0], 1))\n",
    "        else:\n",
    "            expected_reward_ = self.reward_function(\n",
    "                context=context,\n",
    "                action_context=self.action_context,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "\n",
    "        return expected_reward_\n",
    "\n",
    "    def sample_reward_given_expected_reward(\n",
    "        self,\n",
    "        expected_reward: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Sample reward given expected rewards\"\"\"\n",
    "        expected_reward_factual = expected_reward[np.arange(action.shape[0]), action]\n",
    "        if RewardType(self.reward_type) == RewardType.BINARY:\n",
    "            reward = self.random_.binomial(n=1, p=expected_reward_factual)\n",
    "        elif RewardType(self.reward_type) == RewardType.CONTINUOUS:\n",
    "            mean = expected_reward_factual\n",
    "            a = (self.reward_min - mean) / self.reward_std\n",
    "            b = (self.reward_max - mean) / self.reward_std\n",
    "            reward = truncnorm.rvs(\n",
    "                a=a,\n",
    "                b=b,\n",
    "                loc=mean,\n",
    "                scale=self.reward_std,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def sample_reward(self, context: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        check_array(array=context, name=\"context\", expected_dim=2)\n",
    "        check_array(array=action, name=\"action\", expected_dim=1)\n",
    "        if context.shape[0] != action.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Expected `context.shape[0] == action.shape[0]`, but found it False\"\n",
    "            )\n",
    "        if not np.issubdtype(action.dtype, np.integer):\n",
    "            raise ValueError(\"the dtype of action must be a subdtype of int\")\n",
    "\n",
    "        expected_reward_ = self.calc_expected_reward(context)\n",
    "\n",
    "        return self.sample_reward_given_expected_reward(expected_reward_, action)\n",
    "\n",
    "    def synthesize_expected_reward(self, contexts, times):\n",
    "        n_rounds = contexts.shape[0]\n",
    "\n",
    "        # Convert Unix timestamp to a datetime object\n",
    "        finer_time_structure_func = np.vectorize(datetime.datetime.utcfromtimestamp)\n",
    "        dt_objects = finer_time_structure_func(times)\n",
    "\n",
    "        # Assuming dt_objects is a NumPy array of datetime objects\n",
    "        get_day_of_week = np.vectorize(lambda dt: dt.weekday())\n",
    "        days_of_week = get_day_of_week(dt_objects)\n",
    "\n",
    "        finer_time_structure_context = np.zeros(shape=(n_rounds, NUM_DAY_OF_WEEK))\n",
    "\n",
    "        row_indices = np.arange(n_rounds)\n",
    "        column_indices = days_of_week\n",
    "\n",
    "        finer_time_structure_context[row_indices, column_indices] = 1\n",
    "\n",
    "        time_structure_func_vec = np.vectorize(self.time_structure_func)\n",
    "\n",
    "        time_structures = time_structure_func_vec(times)\n",
    "\n",
    "        time_structure_context = np.zeros(shape=(n_rounds, self.num_time_structure))\n",
    "\n",
    "        row_indices = np.arange(n_rounds)\n",
    "        column_indices = time_structures\n",
    "\n",
    "        time_structure_context[row_indices, column_indices] = 1\n",
    "\n",
    "        # Synthetize h(x, t, a)\n",
    "        # if h(x, t, a) is a simple or comlex function\n",
    "\n",
    "        # Initialize h(x, t, a) by zero matrix\n",
    "        h_x_t_a_ = np.zeros((n_rounds, self.n_actions))\n",
    "\n",
    "        # Synthesize each of the componets to synthesize h(x, t, a)\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            h_1_x = (contexts[:, 0:6].sum(axis=1) < 2.5) * self.theta_x[0]\n",
    "            h_1_x += (contexts[:, 7:9].sum(axis=1) < -0.5) * self.theta_x[1]\n",
    "            h_1_x += (contexts[:, 2:5].sum(axis=1) > 2.0) * self.theta_x[2]\n",
    "        else:\n",
    "            h_1_x = contexts @ self.theta_x / self.dim_context\n",
    "\n",
    "        h_2_t = finer_time_structure_context @ self.theta_finer_phi_t\n",
    "\n",
    "        h_3_a = self.action_context @ self.theta_a\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            shrinked_contexts = np.concatenate(\n",
    "                [\n",
    "                    (contexts[:, 0:4].sum(axis=1) < 3).reshape(-1, 1),\n",
    "                    (contexts[:, 2:9].sum(axis=1) > 2.5).reshape(-1, 1),\n",
    "                    (contexts[:, 1:7].sum(axis=1) < 1.5).reshape(-1, 1),\n",
    "                    (contexts[:, 6:10].sum(axis=1) > -1.5).reshape(-1, 1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            h_5_x_a = shrinked_contexts @ self.N_x_a[0:4, :] @ self.action_context\n",
    "        else:\n",
    "            h_5_x_a = contexts @ self.N_x_a @ self.action_context / self.dim_context\n",
    "\n",
    "        h_6_t_a = (\n",
    "            finer_time_structure_context @ self.N_finer_phi_t_a_1 @ self.action_context\n",
    "        )\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            shrinked_contexts = np.concatenate(\n",
    "                [\n",
    "                    (contexts[:, 0:4].sum(axis=1) < 4).reshape(-1, 1),\n",
    "                    (contexts[:, 2:9].sum(axis=1) > 3.5).reshape(-1, 1),\n",
    "                    (contexts[:, 2:5].sum(axis=1) > 1.5).reshape(-1, 1),\n",
    "                    (contexts[:, 5:10].sum(axis=1) < -2.5).reshape(-1, 1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            h_7_x_phi_t_a = np.einsum(\n",
    "                \"ij,jkl->ikl\", shrinked_contexts, self.P_x_finer_phi_t_a[0:4, :, :]\n",
    "            )\n",
    "            h_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", h_7_x_phi_t_a, finer_time_structure_context\n",
    "            )\n",
    "            h_7_x_phi_t_a = h_7_x_phi_t_a @ self.action_context\n",
    "        else:\n",
    "            h_7_x_phi_t_a = (\n",
    "                np.einsum(\"ij,jkl->ikl\", contexts, self.P_x_finer_phi_t_a)\n",
    "                / self.dim_context\n",
    "            )\n",
    "            h_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", h_7_x_phi_t_a, finer_time_structure_context\n",
    "            )\n",
    "            h_7_x_phi_t_a = h_7_x_phi_t_a @ self.action_context\n",
    "\n",
    "        h_x_t_a_ = (\n",
    "            h_1_x[:, np.newaxis]\n",
    "            + h_2_t[:, np.newaxis]\n",
    "            + h_3_a\n",
    "            + h_5_x_a\n",
    "            + h_6_t_a\n",
    "            + h_7_x_phi_t_a\n",
    "        )\n",
    "\n",
    "        # Synthetize g(x, \\phi(t), a)\n",
    "        g_x_phi_t_a_ = np.zeros((n_rounds, self.n_actions))\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            g_1_x = (contexts[:, 0:4].sum(axis=1) < 1.5) * self.psi_x[0]\n",
    "            g_1_x += (contexts[:, 5:9].sum(axis=1) < -0.5) * self.psi_x[1]\n",
    "            g_1_x += (contexts[:, 3:5].sum(axis=1) > 3.0) * self.psi_x[2]\n",
    "            g_1_x += (contexts[:, 6:10].sum(axis=1) < 1.0) * self.psi_x[3]\n",
    "        else:\n",
    "            g_1_x = contexts @ self.psi_x / self.dim_context\n",
    "\n",
    "        g_2_phi_t = time_structure_context @ self.psi_phi_t\n",
    "\n",
    "        g_6_phi_t_a = time_structure_context @ self.M_phi_t_a @ self.action_context\n",
    "\n",
    "        if self.dim_context == 10:\n",
    "            shrinked_contexts = np.concatenate(\n",
    "                [\n",
    "                    (contexts[:, 0:4].sum(axis=1) < 4).reshape(-1, 1),\n",
    "                    (contexts[:, 5:9].sum(axis=1) > 3).reshape(-1, 1),\n",
    "                    (contexts[:, 2:10].sum(axis=1) < -2.5).reshape(-1, 1),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "            g_7_x_phi_t_a = np.einsum(\n",
    "                \"ij,jkl->ikl\", shrinked_contexts, self.P_x_phi_t_a[0:3, :, :]\n",
    "            )\n",
    "            g_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", g_7_x_phi_t_a, time_structure_context\n",
    "            )\n",
    "            g_7_x_phi_t_a = g_7_x_phi_t_a @ self.action_context\n",
    "        else:\n",
    "            g_7_x_phi_t_a = (\n",
    "                np.einsum(\"ij,jkl->ikl\", contexts, self.P_x_phi_t_a) / self.dim_context\n",
    "            )\n",
    "            g_7_x_phi_t_a = np.einsum(\n",
    "                \"ijk,ij->ik\", g_7_x_phi_t_a, time_structure_context\n",
    "            )\n",
    "            g_7_x_phi_t_a = g_7_x_phi_t_a @ self.action_context\n",
    "\n",
    "        # Take the sum of each vector or matrices to consturct h(x, t, a)\n",
    "        g_x_phi_t_a_ = (\n",
    "            g_1_x[:, np.newaxis]\n",
    "            + g_2_phi_t[:, np.newaxis]\n",
    "            + g_6_phi_t_a\n",
    "            + g_7_x_phi_t_a\n",
    "        )\n",
    "\n",
    "        # q(x, t, a) = \\lambda * g(x, \\phi(t), a) + (1 - \\lambda) * h(x, t, a)\n",
    "        expected_reward_ = (\n",
    "            self.lambda_ratio * g_x_phi_t_a_ + (1 - self.lambda_ratio) * h_x_t_a_\n",
    "        )\n",
    "\n",
    "        return g_x_phi_t_a_, h_x_t_a_, expected_reward_\n",
    "\n",
    "    def obtain_batch_bandit_feedback(\n",
    "        self,\n",
    "        n_rounds: int,\n",
    "        evaluation_mode=False,\n",
    "        time_at_evaluation=0,\n",
    "        random_state_for_sampling=None,\n",
    "    ) -> BanditFeedback:\n",
    "        check_scalar(n_rounds, \"n_rounds\", int, min_val=1)\n",
    "\n",
    "        random_for_sample_ = check_random_state(\n",
    "            random_state_for_sampling + self.random_state\n",
    "        )\n",
    "\n",
    "        # Observe time\n",
    "        if evaluation_mode == False:\n",
    "            # Sample time data with size n from the uniform distribution ranging from t_oldest to t_now\n",
    "            times = random_for_sample_.uniform(\n",
    "                self.t_oldest, self.t_now, size=n_rounds\n",
    "            ).astype(int)\n",
    "\n",
    "            times.sort()\n",
    "        else:\n",
    "            # All time are time_at_evaluation\n",
    "            times = np.full(n_rounds, time_at_evaluation)\n",
    "\n",
    "        # Observe context\n",
    "        # Stationary context\n",
    "        if self.sample_non_stationary_context == False:\n",
    "            contexts = random_for_sample_.normal(size=(n_rounds, self.dim_context))\n",
    "\n",
    "        # Non-stationary context\n",
    "        else:\n",
    "            # normalize the time vector\n",
    "            normalized_time = normalize_time(times, self.t_oldest, self.t_future)\n",
    "\n",
    "            time_structure_func_for_context_vec = np.vectorize(\n",
    "                self.time_structure_func_for_context\n",
    "            )\n",
    "\n",
    "            time_structures_for_context = time_structure_func_for_context_vec(times)\n",
    "\n",
    "            time_structure_context_for_context = np.zeros(\n",
    "                shape=(n_rounds, self.num_time_structure_for_context)\n",
    "            )\n",
    "\n",
    "            row_indices = np.arange(n_rounds)\n",
    "            column_indices = time_structures_for_context\n",
    "\n",
    "            time_structure_context_for_context[row_indices, column_indices] = 1\n",
    "\n",
    "            mu_1 = time_structure_context_for_context @ self.gamma\n",
    "\n",
    "            Sigma_1 = 1\n",
    "\n",
    "            mu_2 = self.delta[0] * normalized_time\n",
    "\n",
    "            Sigma_2 = 1\n",
    "\n",
    "            # Augment the mean vector to the matrix\n",
    "            mu_1_mat = mu_1[:, np.newaxis]\n",
    "            mu_1_mat = mu_1_mat * np.ones((1, self.dim_context))\n",
    "\n",
    "            mu_2_mat = mu_2[:, np.newaxis]\n",
    "            mu_2_mat = mu_2_mat * np.ones((1, self.dim_context))\n",
    "\n",
    "            # Sample each of the elements to construct the context\n",
    "            contexts_1 = random_for_sample_.normal(\n",
    "                size=(n_rounds, self.dim_context), loc=mu_1_mat, scale=Sigma_1\n",
    "            )\n",
    "            contexts_2 = random_for_sample_.normal(\n",
    "                size=(n_rounds, self.dim_context), loc=mu_2_mat, scale=Sigma_2\n",
    "            )\n",
    "            # Synthetize the context\n",
    "            contexts = (\n",
    "                self.alpha_ratio * contexts_1 + (1 - self.alpha_ratio) * contexts_2\n",
    "            )\n",
    "\n",
    "        g_x_phi_t_a_, h_x_t_a_, expected_reward_ = self.synthesize_expected_reward(\n",
    "            contexts, times\n",
    "        )\n",
    "\n",
    "        if RewardType(self.reward_type) == RewardType.CONTINUOUS:\n",
    "            # correct expected_reward_, as we use truncated normal distribution here\n",
    "            mean = expected_reward_\n",
    "            a = (self.reward_min - mean) / self.reward_std\n",
    "            b = (self.reward_max - mean) / self.reward_std\n",
    "            expected_reward_ = truncnorm.stats(\n",
    "                a=a, b=b, loc=mean, scale=self.reward_std, moments=\"m\"\n",
    "            )\n",
    "\n",
    "        # calculate the action choice probabilities of the behavior policy\n",
    "        if self.behavior_policy_function is None:\n",
    "            pi_b_logits = expected_reward_\n",
    "        else:\n",
    "            pi_b_logits = self.behavior_policy_function(\n",
    "                context=contexts,\n",
    "                action_context=self.action_context,\n",
    "                random_state=self.random_state,\n",
    "            )\n",
    "        # create some deficient actions based on the value of `n_deficient_actions`\n",
    "        if self.n_deficient_actions > 0:\n",
    "            pi_b = np.zeros_like(pi_b_logits)\n",
    "            n_supported_actions = self.n_actions - self.n_deficient_actions\n",
    "            supported_actions = np.argsort(\n",
    "                self.random_.gumbel(size=(n_rounds, self.n_actions)), axis=1\n",
    "            )[:, ::-1][:, :n_supported_actions]\n",
    "            supported_actions_idx = (\n",
    "                np.tile(np.arange(n_rounds), (n_supported_actions, 1)).T,\n",
    "                supported_actions,\n",
    "            )\n",
    "            pi_b[supported_actions_idx] = softmax(\n",
    "                self.beta * pi_b_logits[supported_actions_idx]\n",
    "            )\n",
    "        else:\n",
    "            pi_b = softmax(self.beta * pi_b_logits)\n",
    "        # sample actions for each round based on the behavior policy\n",
    "        actions = sample_action_fast(pi_b, random_state=self.random_state)\n",
    "\n",
    "        # sample rewards based on the context and action\n",
    "        rewards = self.sample_reward_given_expected_reward(expected_reward_, actions)\n",
    "\n",
    "        return dict(\n",
    "            n_rounds=n_rounds,\n",
    "            n_actions=self.n_actions,\n",
    "            context=contexts,\n",
    "            time=times,\n",
    "            action_context=self.action_context,\n",
    "            action=actions,\n",
    "            position=None,\n",
    "            reward=rewards,\n",
    "            expected_reward=expected_reward_,\n",
    "            g_x_phi_t_a=g_x_phi_t_a_,\n",
    "            h_x_t_a=h_x_t_a_,\n",
    "            pi_b=pi_b[:, :, np.newaxis],\n",
    "            pscore=pi_b[np.arange(n_rounds), actions],\n",
    "        )\n",
    "\n",
    "    def calc_ground_truth_policy_value(\n",
    "        self, expected_reward: np.ndarray, action_dist: np.ndarray\n",
    "    ) -> float:\n",
    "        return np.average(expected_reward, weights=action_dist, axis=1).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
