{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm import tqdm\n",
    "\n",
    "import conf\n",
    "from synthetic_time import (\n",
    "    SyntheticBanditWithTimeDataset, \n",
    "    SECONDS_PER_DAY\n",
    ")\n",
    "from policy import gen_eps_greedy\n",
    "from ope import run_ope\n",
    "from utils import show_hyperparameters\n",
    "from logging import getLogger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:The current working directory is c:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\n"
     ]
    }
   ],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.info(f\"The current working directory is {Path().cwd()}\")\n",
    "\n",
    "# log path\n",
    "log_path = Path(\"./varying_n_trains_data\")\n",
    "df_path = log_path / \"df\"\n",
    "df_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# START hyperparameters #################\n",
      "### About Seeds and Number of Samples ###\n",
      "number of seeds = 20\n",
      "number of seeds for time at evaluation = 20\n",
      "number of training samples (n) = 1000\n",
      "number of test samples = 10000\n",
      "\n",
      "### About Time Structure ###\n",
      "number of true time structures for reward (|C_r|) = 8\n",
      "strength of time structure for reward (lambda) = 0.5\n",
      "\n",
      "### About Prognosticator ###\n",
      "list of time features for Prognosticator = [<function fourier_scalar at 0x0000018774942A60>]\n",
      "optimality of the data driven feature selection for Prognosticator = True\n",
      "number of time features for Prognosticator = 3\n",
      "list of the numbers of time features for Prognosticator = range(3, 8, 2)\n",
      "\n",
      "### About Logged Data Collection Period and Evaluation Period ###\n",
      "time when we start collecting the logged data = 2022-01-01 00:00:00\n",
      "time when we finish collecting the logged data = 2022-12-31 23:59:59\n",
      "time when we start evaluating a target policy = 2023-01-01 00:00:00\n",
      "time when we finish evaluating a target policy = 2023-12-31 23:59:59\n",
      "future time = 2024-01-01 00:00:00\n",
      "\n",
      "### About Parameters for Data Generating Process ###\n",
      "number of actions (|A|) = 10\n",
      "dimension of context (d_x) = 10\n",
      "number of users = None\n",
      "behavior policy optimality (beta) = 0.1\n",
      "target policy optimality (epsilon) = 0.2\n",
      "\n",
      "### About Varying Parameters ###\n",
      "list of the numbers of training samples (n) = [500, 1000, 2000, 4000]\n",
      "list of the strengths of time structure for reward (lambda) = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "list of the numbers of candidate time structures for reward = range(2, 17, 2)\n",
      "################# END hyperparameters #################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:   5%|▌         | 1/20 [00:01<00:20,  1.10s/it]c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:  10%|█         | 2/20 [00:02<00:19,  1.07s/it]c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:  15%|█▌        | 3/20 [00:03<00:17,  1.06s/it]c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:  20%|██        | 4/20 [00:04<00:16,  1.05s/it]c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:  25%|██▌       | 5/20 [00:05<00:15,  1.05s/it]c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:  30%|███       | 6/20 [00:06<00:14,  1.06s/it]c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n",
      "h = 0, number of samples in the logged data = 500, n_seeds_for_time_eval_sampling = 0:  30%|███       | 6/20 [00:07<00:16,  1.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 98\u001b[0m\n\u001b[0;32m     93\u001b[0m     days_per_time_structure \u001b[38;5;241m=\u001b[39m NUM_DAYS_IN_ONE_CYCLE \u001b[38;5;241m/\u001b[39m dataset\u001b[38;5;241m.\u001b[39mnum_time_structure\n\u001b[0;32m     95\u001b[0m     num_time_structure_from_t_now_to_time_at_evaluation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mceil(days_after_logged_data \u001b[38;5;241m/\u001b[39m days_per_time_structure)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mrun_ope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_at_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_at_evaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimated_policy_value_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimated_policy_value_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_bandit_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_bandit_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_dist_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction_dist_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_true_time_structure_for_OPFV_reward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_true_time_structure_for_OPFV_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_true_time_structure_for_OPFV_for_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_episodes_for_Prognosticator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_episodes_for_Prognosticator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_time_structure_from_t_now_to_time_at_evaluation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_time_structure_from_t_now_to_time_at_evaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrue_policy_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpolicy_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflag_Prognosticator_optimality\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflag_Prognosticator_optimality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_features_for_Prognosticator_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_features_for_Prognosticator_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflag_include_DM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflag_include_DM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflag_calculate_data_driven_OPFV\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflag_calculate_data_driven_OPFV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcandidate_num_time_structure_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidate_num_time_structure_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m result_df \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    118\u001b[0m     DataFrame(DataFrame(estimated_policy_value_list)\u001b[38;5;241m.\u001b[39mstack())\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    122\u001b[0m result_df[x] \u001b[38;5;241m=\u001b[39m n_rounds\n",
      "File \u001b[1;32mc:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\ope.py:277\u001b[0m, in \u001b[0;36mrun_ope\u001b[1;34m(dataset, round, time_at_evaluation, estimated_policy_value_list, val_bandit_data, action_dist_val, num_episodes_for_Prognosticator, num_time_structure_from_t_now_to_time_at_evaluation, num_true_time_structure_for_OPFV_reward, num_true_time_structure_for_OPFV_for_context, eps, flag_calulate_robust_OPFV, flag_Prognosticator_optimality, num_features_for_Prognosticator_list, true_policy_value, flag_include_DM, flag_calculate_data_driven_OPFV, candidate_num_time_structure_list)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcandidate_phi_scalar_func\u001b[39m(unix_time):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unix_time_to_time_structure_n_tree(\n\u001b[0;32m    274\u001b[0m         unix_time, candidate_num_time_structure\n\u001b[0;32m    275\u001b[0m     )\n\u001b[1;32m--> 277\u001b[0m hat_f_x_t_a, hat_f_x_t_a_at_eval \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_hat_f_train_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidate_phi_scalar_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_bandit_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_at_eval_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m candidate_value_round_rewards \u001b[38;5;241m=\u001b[39m OPFV(\n\u001b[0;32m    285\u001b[0m     phi_scalar_func\u001b[38;5;241m=\u001b[39mcandidate_phi_scalar_func,  \u001b[38;5;66;03m# \\phi_r(t)\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     phi_scalar_func_for_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# \\phi_x(t)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     P_phi_t_true_for_context_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# P(\\phi_{x, r}(t))\u001b[39;00m\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m candidate_estimated_value_list\u001b[38;5;241m.\u001b[39mappend(candidate_value_round_rewards\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32mc:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\utils.py:113\u001b[0m, in \u001b[0;36mcalculate_hat_f_train_and_eval\u001b[1;34m(phi_scalar_func_for_OPFV, val_bandit_data, dataset, time_at_eval_vec, round)\u001b[0m\n\u001b[0;32m     98\u001b[0m reg_model_time_structure \u001b[38;5;241m=\u001b[39m RegressionModelTimeStructure(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Number of actions.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     n_actions\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mn_actions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m     ),\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Fit the regression model on given logged bandit data and estimate the expected rewards on the same data.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Returns\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#  g_hat: array-like, shape (n_rounds, n_actions, len_list)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#  Expected rewards of new data estimated by the regression model.\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m hat_g_x_phi_t_a \u001b[38;5;241m=\u001b[39m \u001b[43mreg_model_time_structure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_bandit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# context; x\u001b[39;49;00m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_structure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# time structure: phi(t)\u001b[39;49;00m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_bandit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# action; a\u001b[39;49;00m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_bandit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# reward; r\u001b[39;49;00m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12345\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m hat_g_x_phi_t_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(hat_g_x_phi_t_a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    124\u001b[0m hat_f_x_t_a \u001b[38;5;241m=\u001b[39m hat_g_x_phi_t_a\n",
      "File \u001b[1;32mc:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\regression_model_time.py:369\u001b[0m, in \u001b[0;36mRegressionModelTimeStructure.fit_predict\u001b[1;34m(self, context, time_structure, action, reward, pscore, position, action_dist, n_folds, random_state)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, test_idx \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(context):\n\u001b[0;32m    366\u001b[0m     action_dist_tr \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    367\u001b[0m         action_dist[train_idx] \u001b[38;5;28;01mif\u001b[39;00m action_dist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m action_dist\n\u001b[0;32m    368\u001b[0m     )\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_structure\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpscore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dist_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     q_hat[test_idx, :, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m    379\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext[test_idx], time_structure\u001b[38;5;241m=\u001b[39mtime_structure[test_idx]\n\u001b[0;32m    380\u001b[0m     )\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_hat\n",
      "File \u001b[1;32mc:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\regression_model_time.py:279\u001b[0m, in \u001b[0;36mRegressionModelTimeStructure.fit\u001b[1;34m(self, context, time_structure, action, reward, pscore, position, action_dist)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# train the base model according to the given `fitting method`\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitting_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     action_dist_at_pos \u001b[38;5;241m=\u001b[39m action_dist[np\u001b[38;5;241m.\u001b[39marange(n), action, pos_][idx]\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\sklearn\\tree\\_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m ):\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Obtain the unix time when we start the evaluation of a target policy\n",
    "time_at_evaluation_start = conf.time_at_evaluation\n",
    "# Calculate the number of days in one cycle of given time structure function \\phi(t)\n",
    "NUM_DAYS_IN_ONE_CYCLE = 365\n",
    "# Determine the unix time when we end the evaluation of a target policy \n",
    "time_at_evaluation_end_datetime = datetime.datetime.fromtimestamp(time_at_evaluation_start) + datetime.timedelta(days=NUM_DAYS_IN_ONE_CYCLE * conf.num_cycles_in_evaluation_period) - datetime.timedelta(seconds=1)\n",
    "time_at_evaluation_end = int(datetime.datetime.timestamp(time_at_evaluation_end_datetime))\n",
    "\n",
    "# Show hyperparameters\n",
    "show_hyperparameters(time_at_evaluation_start=time_at_evaluation_start, \n",
    "                     time_at_evaluation_end=time_at_evaluation_end, \n",
    "                     flag_show_time_at_evaluation=True)\n",
    "\n",
    "x = \"n_rounds\"\n",
    "xlabel = \"number of samples in the logged data\"\n",
    "xticklabels = conf.n_rounds_list\n",
    "\n",
    "result_df_list = []\n",
    "\n",
    "for h in range(conf.n_seeds_all):\n",
    "    for n_rounds in conf.n_rounds_list: \n",
    "\n",
    "        dataset = SyntheticBanditWithTimeDataset(\n",
    "            n_actions=conf.n_actions, \n",
    "            dim_context=conf.dim_context,\n",
    "            n_users=conf.n_users, \n",
    "            t_oldest = conf.t_oldest,\n",
    "            t_now = conf.t_now,\n",
    "            t_future = conf.t_future,\n",
    "            beta = conf.beta, \n",
    "            reward_std = conf.reward_std, \n",
    "            num_time_structure=conf.num_time_structure_for_logged_data, \n",
    "            lambda_ratio = conf.lambda_ratio, \n",
    "            flag_simple_reward = conf.flag_simple_reward, \n",
    "            g_coef=conf.g_coef, \n",
    "            h_coef=conf.h_coef, \n",
    "            random_state=conf.random_state + h * 10,\n",
    "        )\n",
    "\n",
    "        for s in range(conf.n_seeds_for_time_eval_sampling):\n",
    "\n",
    "            estimated_policy_value_list = []   \n",
    "\n",
    "\n",
    "            # Obtain random state\n",
    "            random_ = check_random_state(s + h * 10)\n",
    "            # Sample the time at evaluation from given distribution (uniform)\n",
    "            time_at_evaluation = random_.uniform(time_at_evaluation_start, time_at_evaluation_end, size=1).astype(int)\n",
    "\n",
    "            ### test bandit data is used to approximate the ground-truth policy value\n",
    "            test_bandit_data = dataset.obtain_batch_bandit_feedback(\n",
    "                n_rounds=conf.num_test, \n",
    "                evaluation_mode=True, \n",
    "                time_at_evaluation=time_at_evaluation, \n",
    "                random_state_for_sampling= s + h * 10\n",
    "            )\n",
    "\n",
    "            # Generate an evaluation policy via the epsilon-greedy rule\n",
    "            action_dist_test = gen_eps_greedy(\n",
    "                expected_reward=test_bandit_data[\"expected_reward\"],\n",
    "                is_optimal=True,\n",
    "                eps=conf.eps,\n",
    "            )\n",
    "\n",
    "            # actulal policy value \n",
    "            policy_value = dataset.calc_ground_truth_policy_value(\n",
    "                expected_reward=test_bandit_data[\"expected_reward\"],\n",
    "                action_dist=action_dist_test,\n",
    "            )\n",
    "        \n",
    "            for _ in tqdm(range(conf.n_seeds), desc=f\"h = {h}, {xlabel} = {n_rounds}, n_seeds_for_time_eval_sampling = {s}\"):\n",
    "\n",
    "                \n",
    "                ## generate validation data\n",
    "                val_bandit_data = dataset.obtain_batch_bandit_feedback(\n",
    "                    n_rounds=n_rounds, \n",
    "                    evaluation_mode=False, \n",
    "                    random_state_for_sampling = _ + s * 10 + n_rounds + h * 100\n",
    "                )\n",
    "                \n",
    "                ## make decisions on validation data\n",
    "                action_dist_val = gen_eps_greedy(\n",
    "                    expected_reward=val_bandit_data[\"expected_reward\"],\n",
    "                    is_optimal=True,\n",
    "                    eps=conf.eps,\n",
    "                )\n",
    "\n",
    "                days_after_logged_data = (time_at_evaluation - dataset.t_now) // SECONDS_PER_DAY\n",
    "\n",
    "                days_per_time_structure = NUM_DAYS_IN_ONE_CYCLE / dataset.num_time_structure\n",
    "\n",
    "                num_time_structure_from_t_now_to_time_at_evaluation = np.ceil(days_after_logged_data / days_per_time_structure).astype(int)\n",
    "                \n",
    "                \n",
    "                run_ope(dataset=dataset, \n",
    "                        round = _ + s * 10 + h * 100, \n",
    "                        time_at_evaluation=time_at_evaluation, \n",
    "                        estimated_policy_value_list=estimated_policy_value_list, \n",
    "                        val_bandit_data = val_bandit_data, \n",
    "                        action_dist_val = action_dist_val, \n",
    "                        num_true_time_structure_for_OPFV_reward = conf.num_true_time_structure_for_OPFV_reward,\n",
    "                        num_true_time_structure_for_OPFV_for_context = None, \n",
    "                        num_episodes_for_Prognosticator = conf.num_episodes_for_Prognosticator, \n",
    "                        num_time_structure_from_t_now_to_time_at_evaluation = num_time_structure_from_t_now_to_time_at_evaluation, \n",
    "                        eps=conf.eps, \n",
    "                        true_policy_value = policy_value, \n",
    "                        flag_Prognosticator_optimality = conf.flag_Prognosticator_optimality, \n",
    "                        num_features_for_Prognosticator_list = conf.num_features_for_Prognosticator_list,\n",
    "                        flag_include_DM=conf.flag_include_DM, \n",
    "                        flag_calculate_data_driven_OPFV = conf.flag_calculate_data_driven_OPFV, \n",
    "                        candidate_num_time_structure_list = conf.candidate_num_time_structure_list, \n",
    "                        )\n",
    "            \n",
    "            result_df = (\n",
    "                DataFrame(DataFrame(estimated_policy_value_list).stack())\n",
    "                .reset_index(1)\n",
    "                .rename(columns={\"level_1\": \"est\", 0: \"value\"})\n",
    "            )\n",
    "            result_df[x] = n_rounds\n",
    "            result_df[\"se\"] = (result_df.value - policy_value) ** 2\n",
    "            result_df[\"bias\"] = 0\n",
    "            result_df[\"variance\"] = 0\n",
    "            sample_mean = DataFrame(result_df[result_df[\"est\"] != \"V_t\"].groupby([\"est\"]).mean().value).reset_index()\n",
    "            for est_ in sample_mean[\"est\"]:\n",
    "                estimates = result_df.loc[result_df[\"est\"] == est_, \"value\"].values\n",
    "                mean_estimates = sample_mean.loc[sample_mean[\"est\"] == est_, \"value\"].values\n",
    "                mean_estimates = np.ones_like(estimates) * mean_estimates\n",
    "                result_df.loc[result_df[\"est\"] == est_, \"bias\"] = (\n",
    "                    policy_value - mean_estimates\n",
    "                ) ** 2\n",
    "                result_df.loc[result_df[\"est\"] == est_, \"variance\"] = (\n",
    "                    estimates - mean_estimates\n",
    "                ) ** 2\n",
    "            result_df_list.append(result_df)\n",
    "\n",
    "\n",
    "# aggregate all results\n",
    "result_df = pd.concat(result_df_list).reset_index(level=0)\n",
    "result_df.to_csv(df_path / \"result_df.csv\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"execution time: {elapsed_time / 60} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of policy failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\policy.py\", line 38, in <module>\n",
      "    available_actions: np.ndarray | None = None,\n",
      "TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\n",
      "]\n",
      "[autoreload of utils failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\utils.py\", line 141, in <module>\n",
      "    from regression_model_time import (\n",
      "ImportError: cannot import name 'RegressionModelTimeWithEmbedding' from 'regression_model_time' (c:\\Users\\taish\\kdd2025-opfv\\src\\synthetic\\F-OPE\\regression_model_time.py)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# ================== Imports ==================\n",
    "import time, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import conf\n",
    "from utils import show_hyperparameters\n",
    "from policy import gen_eps_greedy,   # 既存\n",
    "# マスク版はあれば使う（無ければ後でフォールバック）\n",
    "try:\n",
    "    from policy import gen_eps_greedy_masked\n",
    "except Exception:\n",
    "    gen_eps_greedy_masked = None\n",
    "\n",
    "# データセット：既存 & 動的\n",
    "from synthetic_time import SyntheticBanditWithTimeDataset\n",
    "try:\n",
    "    # あなたの新クラス（ファイル配置に合わせて適宜 import してください）\n",
    "    from synthetic_time_dynamic import DynamicActionBanditWithTime  # 例\n",
    "except Exception:\n",
    "    try:\n",
    "        # synthetic_time 内に定義している場合はこちらで拾う\n",
    "        from synthetic_time import DynamicActionBanditWithTime\n",
    "    except Exception:\n",
    "        DynamicActionBanditWithTime = None  # フォールバック用\n",
    "\n",
    "# ランナー：私用（動的）→ 既存 の順で採用\n",
    "try:\n",
    "    from ope_dynamic import run_ope_masked as run_ope_fn\n",
    "except Exception:\n",
    "    from ope import run_ope as run_ope_fn  # 既存ランナーにフォールバック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランナー：私用（動的）→ 既存 の順で採用\n",
    "try:\n",
    "    import importlib\n",
    "    import ope_dynamic                      # ← ファイル名と一致\n",
    "    importlib.reload(ope_dynamic)           # ノートブックでの上書きに備えて\n",
    "    from ope_dynamic import run_ope_masked as run_ope_fn\n",
    "except Exception as e:\n",
    "    print(\"Falling back to standard run_ope. Reason:\", e)\n",
    "    from ope import run_ope as run_ope_fn       # 既存ランナー\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Configs & helpers ==================\n",
    "SECONDS_PER_DAY = 86400\n",
    "NUM_DAYS_IN_ONE_CYCLE = 365  # φ(t) の1周期（日数）—必要に応じて置き換え\n",
    "\n",
    "df_path = Path(\"./results\")\n",
    "df_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dynamic dataset を作るヘルパ\n",
    "def build_dataset(seed_offset: int = 0):\n",
    "    if getattr(conf, \"use_dynamic_action_dataset\", False) and DynamicActionBanditWithTime is not None:\n",
    "        kwargs = dict(\n",
    "            n_actions=conf.n_actions,\n",
    "            dim_context=conf.dim_context,\n",
    "            n_users=conf.n_users,\n",
    "            t_oldest=conf.t_oldest,\n",
    "            t_now=conf.t_now,\n",
    "            t_future=conf.t_future,\n",
    "            beta=conf.beta,\n",
    "            reward_std=conf.reward_std,\n",
    "            num_time_structure=conf.num_time_structure_for_logged_data,\n",
    "            lambda_ratio=conf.lambda_ratio,\n",
    "            flag_simple_reward=conf.flag_simple_reward,\n",
    "            g_coef=conf.g_coef,\n",
    "            h_coef=conf.h_coef,\n",
    "            random_state=conf.random_state + seed_offset,\n",
    "        )\n",
    "        # 可用性の指定（conf 側の追加パラメタを利用）\n",
    "        if getattr(conf, \"use_availability_func\", False):\n",
    "            kwargs[\"availability_func\"] = getattr(conf, \"availability_func_weekly_stair\")\n",
    "        else:\n",
    "            # birth/death を conf から渡す（存在すれば）\n",
    "            if hasattr(conf, \"action_birth_time\"):\n",
    "                kwargs[\"action_birth_time\"] = conf.action_birth_time\n",
    "            if hasattr(conf, \"action_death_time\") and conf.action_death_time is not None:\n",
    "                kwargs[\"action_death_time\"] = conf.action_death_time\n",
    "\n",
    "        return DynamicActionBanditWithTime(**kwargs)\n",
    "    else:\n",
    "        # 既存の定常（時間あり）ベースライン\n",
    "        return SyntheticBanditWithTimeDataset(\n",
    "            n_actions=conf.n_actions,\n",
    "            dim_context=conf.dim_context,\n",
    "            n_users=conf.n_users,\n",
    "            t_oldest=conf.t_oldest,\n",
    "            t_now=conf.t_now,\n",
    "            t_future=conf.t_future,\n",
    "            beta=conf.beta,\n",
    "            reward_std=conf.reward_std,\n",
    "            num_time_structure=conf.num_time_structure_for_logged_data,\n",
    "            lambda_ratio=conf.lambda_ratio,\n",
    "            flag_simple_reward=conf.flag_simple_reward,\n",
    "            g_coef=conf.g_coef,\n",
    "            h_coef=conf.h_coef,\n",
    "            random_state=conf.random_state + seed_offset,\n",
    "        )\n",
    "\n",
    "# 可用マスクを安全に取得\n",
    "def get_avail_from_feedback_or_dataset(dataset, times_vec, feedback=None, fallback_shape=None):\n",
    "    if feedback is not None and \"available_actions\" in feedback:\n",
    "        return feedback[\"available_actions\"].astype(bool)\n",
    "    # dataset 側の _availability を使う\n",
    "    if hasattr(dataset, \"_availability\") and callable(dataset._availability):\n",
    "        return dataset._availability(times_vec).astype(bool)\n",
    "    # 全 True フォールバック\n",
    "    if fallback_shape is None and feedback is not None and \"expected_reward\" in feedback:\n",
    "        fallback_shape = feedback[\"expected_reward\"].shape\n",
    "    if fallback_shape is None:\n",
    "        raise ValueError(\"fallback_shape is required when neither feedback nor dataset provide availability.\")\n",
    "    return np.ones(fallback_shape, dtype=bool)\n",
    "\n",
    "\n",
    "# ================== Experiment ==================\n",
    "start_time = time.time()\n",
    "\n",
    "# 将来評価期間の開始・終了\n",
    "time_at_evaluation_start = conf.time_at_evaluation\n",
    "time_at_evaluation_end_datetime = datetime.datetime.fromtimestamp(time_at_evaluation_start) \\\n",
    "    + datetime.timedelta(days=NUM_DAYS_IN_ONE_CYCLE * conf.num_cycles_in_evaluation_period) \\\n",
    "    - datetime.timedelta(seconds=1)\n",
    "time_at_evaluation_end = int(datetime.datetime.timestamp(time_at_evaluation_end_datetime))\n",
    "\n",
    "# ハイパラ表示\n",
    "show_hyperparameters(\n",
    "    time_at_evaluation_start=time_at_evaluation_start,\n",
    "    time_at_evaluation_end=time_at_evaluation_end,\n",
    "    flag_show_time_at_evaluation=True,\n",
    ")\n",
    "\n",
    "x = \"n_rounds\"\n",
    "xlabel = \"number of samples in the logged data\"\n",
    "xticklabels = conf.n_rounds_list\n",
    "\n",
    "result_df_list = []\n",
    "\n",
    "for h in range(conf.n_seeds_all):\n",
    "    for n_rounds in conf.n_rounds_list:\n",
    "\n",
    "        # データセット（seed をずらす）\n",
    "        dataset = build_dataset(seed_offset=h * 10)\n",
    "\n",
    "        for s in range(conf.n_seeds_for_time_eval_sampling):\n",
    "\n",
    "            estimated_policy_value_list = []\n",
    "\n",
    "            # 評価時刻 t' を一様にサンプリング\n",
    "            random_ = check_random_state(s + h * 10)\n",
    "            time_at_evaluation = random_.uniform(\n",
    "                time_at_evaluation_start, time_at_evaluation_end, size=1\n",
    "            ).astype(int)\n",
    "            time_at_evaluation_int = int(time_at_evaluation.item())  # 安全のため int 化\n",
    "\n",
    "            # === 真値近似用の test データ（全て t' ）===\n",
    "            test_bandit_data = dataset.obtain_batch_bandit_feedback(\n",
    "                n_rounds=conf.num_test,\n",
    "                evaluation_mode=True,\n",
    "                time_at_evaluation=time_at_evaluation_int,\n",
    "                random_state_for_sampling=s + h * 10,\n",
    "            )\n",
    "\n",
    "            # 将来時刻の可用マスク（Dynamic 環境なら存在）\n",
    "            avail_test = get_avail_from_feedback_or_dataset(\n",
    "                dataset=dataset,\n",
    "                times_vec=np.full(conf.num_test, time_at_evaluation_int, dtype=int),\n",
    "                feedback=test_bandit_data,\n",
    "                fallback_shape=test_bandit_data[\"expected_reward\"].shape,\n",
    "            )\n",
    "\n",
    "            # 評価方策（将来 t'）— マスク版が使えるならそれを優先\n",
    "            if gen_eps_greedy_masked is not None and getattr(conf, \"use_masked_policy\", True):\n",
    "                action_dist_test = gen_eps_greedy_masked(\n",
    "                    expected_reward=test_bandit_data[\"expected_reward\"],\n",
    "                    eps=conf.eps,\n",
    "                    is_optimal=True,\n",
    "                    available_actions=avail_test,\n",
    "                )\n",
    "            else:\n",
    "                action_dist_test = gen_eps_greedy(\n",
    "                    expected_reward=test_bandit_data[\"expected_reward\"],\n",
    "                    is_optimal=True,\n",
    "                    eps=conf.eps,\n",
    "                )\n",
    "\n",
    "            # 真値 V_t（Dynamic 環境では可用集合で再正規化）\n",
    "            try:\n",
    "                policy_value = dataset.calc_ground_truth_policy_value(\n",
    "                    expected_reward=test_bandit_data[\"expected_reward\"],\n",
    "                    action_dist=action_dist_test,\n",
    "                    available_actions=avail_test,  # 受け取れる実装なら渡す\n",
    "                )\n",
    "            except TypeError:\n",
    "                policy_value = np.average(\n",
    "                    test_bandit_data[\"expected_reward\"],\n",
    "                    weights=action_dist_test,\n",
    "                    axis=1,\n",
    "                ).mean()\n",
    "\n",
    "            # === 推定を複数 seed で回す ===\n",
    "            for _ in tqdm(range(conf.n_seeds), desc=f\"h = {h}, {xlabel} = {n_rounds}, n_seeds_for_time_eval_sampling = {s}\"):\n",
    "\n",
    "                # 検証データ（ログ）\n",
    "                val_bandit_data = dataset.obtain_batch_bandit_feedback(\n",
    "                    n_rounds=n_rounds,\n",
    "                    evaluation_mode=False,\n",
    "                    random_state_for_sampling=_ + s * 10 + n_rounds + h * 100,\n",
    "                )\n",
    "\n",
    "                # 検証時の評価方策（既存どおり）\n",
    "                action_dist_val = gen_eps_greedy(\n",
    "                    expected_reward=val_bandit_data[\"expected_reward\"],\n",
    "                    is_optimal=True,\n",
    "                    eps=conf.eps,\n",
    "                )\n",
    "\n",
    "                # t_now から t' までに跨る φ のステップ数\n",
    "                days_after_logged_data = (time_at_evaluation_int - dataset.t_now) // SECONDS_PER_DAY\n",
    "                days_per_time_structure = NUM_DAYS_IN_ONE_CYCLE / dataset.num_time_structure\n",
    "                num_time_structure_from_t_now_to_time_at_evaluation = int(\n",
    "                    np.ceil(days_after_logged_data / days_per_time_structure)\n",
    "                )\n",
    "\n",
    "                # ランナー呼び出し（私用 run_ope_dynamic があればそちらを使用）\n",
    "                run_ope_fn(\n",
    "                    dataset=dataset,\n",
    "                    round=_ + s * 10 + h * 100,\n",
    "                    time_at_evaluation=time_at_evaluation_int,\n",
    "                    estimated_policy_value_list=estimated_policy_value_list,\n",
    "                    val_bandit_data=val_bandit_data,\n",
    "                    action_dist_val=action_dist_val,\n",
    "                    num_true_time_structure_for_OPFV_reward=conf.num_true_time_structure_for_OPFV_reward,\n",
    "                    num_true_time_structure_for_OPFV_for_context=None,\n",
    "                    num_episodes_for_Prognosticator=conf.num_episodes_for_Prognosticator,\n",
    "                    num_time_structure_from_t_now_to_time_at_evaluation=num_time_structure_from_t_now_to_time_at_evaluation,\n",
    "                    eps=conf.eps,\n",
    "                    true_policy_value=policy_value,\n",
    "                    flag_Prognosticator_optimality=conf.flag_Prognosticator_optimality,\n",
    "                    num_features_for_Prognosticator_list=conf.num_features_for_Prognosticator_list,\n",
    "                    flag_include_DM=conf.flag_include_DM,\n",
    "                    flag_calculate_data_driven_OPFV=conf.flag_calculate_data_driven_OPFV,\n",
    "                    candidate_num_time_structure_list=conf.candidate_num_time_structure_list,\n",
    "                )\n",
    "\n",
    "            # 推定結果を DataFrame 化\n",
    "            result_df = (\n",
    "                DataFrame(DataFrame(estimated_policy_value_list).stack())\n",
    "                .reset_index(1)\n",
    "                .rename(columns={\"level_1\": \"est\", 0: \"value\"})\n",
    "            )\n",
    "            result_df[x] = n_rounds\n",
    "            result_df[\"se\"] = (result_df.value - policy_value) ** 2\n",
    "            result_df[\"bias\"] = 0.0\n",
    "            result_df[\"variance\"] = 0.0\n",
    "\n",
    "            sample_mean = (\n",
    "                DataFrame(result_df[result_df[\"est\"] != \"V_t\"]\n",
    "                          .groupby([\"est\"])\n",
    "                          .mean()\n",
    "                          .value)\n",
    "                .reset_index()\n",
    "            )\n",
    "            for est_ in sample_mean[\"est\"]:\n",
    "                estimates = result_df.loc[result_df[\"est\"] == est_, \"value\"].values\n",
    "                mean_est = sample_mean.loc[sample_mean[\"est\"] == est_, \"value\"].values[0]\n",
    "                mean_vec = np.ones_like(estimates) * mean_est\n",
    "                result_df.loc[result_df[\"est\"] == est_, \"bias\"] = (policy_value - mean_vec) ** 2\n",
    "                result_df.loc[result_df[\"est\"] == est_, \"variance\"] = (estimates - mean_vec) ** 2\n",
    "\n",
    "            result_df_list.append(result_df)\n",
    "\n",
    "# 集計＆保存\n",
    "result_df = pd.concat(result_df_list).reset_index(level=0)\n",
    "result_df.to_csv(df_path / \"result_df.csv\", index=False)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"execution time: {elapsed_time / 60:.2f} mins\")\n",
    "print(f\"Saved: {df_path / 'result_df.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# START hyperparameters #################\n",
      "### About Seeds and Number of Samples ###\n",
      "number of seeds = 20\n",
      "number of seeds for time at evaluation = 20\n",
      "number of training samples (n) = 1000\n",
      "number of test samples = 10000\n",
      "\n",
      "### About Time Structure ###\n",
      "number of true time structures for reward (|C_r|) = 8\n",
      "strength of time structure for reward (lambda) = 0.5\n",
      "\n",
      "### About Prognosticator ###\n",
      "list of time features for Prognosticator = [<function fourier_scalar at 0x0000018774942A60>]\n",
      "optimality of the data driven feature selection for Prognosticator = True\n",
      "number of time features for Prognosticator = 3\n",
      "list of the numbers of time features for Prognosticator = range(3, 8, 2)\n",
      "\n",
      "### About Logged Data Collection Period and Evaluation Period ###\n",
      "time when we start collecting the logged data = 2022-01-01 00:00:00\n",
      "time when we finish collecting the logged data = 2022-12-31 23:59:59\n",
      "time when we start evaluating a target policy = 2023-01-01 00:00:00\n",
      "time when we finish evaluating a target policy = 2023-12-31 23:59:59\n",
      "future time = 2024-01-01 00:00:00\n",
      "\n",
      "### About Parameters for Data Generating Process ###\n",
      "number of actions (|A|) = 10\n",
      "dimension of context (d_x) = 10\n",
      "number of users = None\n",
      "behavior policy optimality (beta) = 0.1\n",
      "target policy optimality (epsilon) = 0.2\n",
      "\n",
      "### About Varying Parameters ###\n",
      "list of the numbers of training samples (n) = [500, 1000, 2000, 4000]\n",
      "list of the strengths of time structure for reward (lambda) = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "list of the numbers of candidate time structures for reward = range(2, 17, 2)\n",
      "################# END hyperparameters #################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_eval: 2023-05-17 16:58:16 | n_rounds: 200 | num_test: 1000\n",
      "True V_t: 14.229388574845641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>est</th>\n",
       "      <th>value</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPS</td>\n",
       "      <td>13.048208</td>\n",
       "      <td>1.395187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR</td>\n",
       "      <td>13.698106</td>\n",
       "      <td>0.282262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prognosticator</td>\n",
       "      <td>10.126804</td>\n",
       "      <td>16.831198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPFV-masked</td>\n",
       "      <td>13.802855</td>\n",
       "      <td>0.181931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPFV</td>\n",
       "      <td>13.802855</td>\n",
       "      <td>0.181931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data-driven OPFV</td>\n",
       "      <td>12.778959</td>\n",
       "      <td>2.103745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V_t</td>\n",
       "      <td>14.229389</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                est      value         SE\n",
       "0               IPS  13.048208   1.395187\n",
       "0                DR  13.698106   0.282262\n",
       "0    Prognosticator  10.126804  16.831198\n",
       "0       OPFV-masked  13.802855   0.181931\n",
       "0              OPFV  13.802855   0.181931\n",
       "0  data-driven OPFV  12.778959   2.103745\n",
       "0               V_t  14.229389   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 1.68s\n"
     ]
    }
   ],
   "source": [
    "# ===== 超小型設定（conf を上書きせず、このセル内だけで使う） =====\n",
    "SECONDS_PER_DAY = 86400\n",
    "NUM_DAYS_IN_ONE_CYCLE = 365\n",
    "\n",
    "# 1回だけ回す\n",
    "n_rounds_min     = 200\n",
    "num_test_min     = 1000\n",
    "n_inner_seeds    = 1     # 既存コードの conf.n_seeds 相当\n",
    "seed_offset      = 0\n",
    "\n",
    "# 将来評価の開始〜終了（1周期だけ）\n",
    "time_at_evaluation_start = conf.time_at_evaluation\n",
    "time_at_evaluation_end_datetime = datetime.datetime.fromtimestamp(time_at_evaluation_start) \\\n",
    "    + datetime.timedelta(days=NUM_DAYS_IN_ONE_CYCLE * conf.num_cycles_in_evaluation_period) \\\n",
    "    - datetime.timedelta(seconds=1)\n",
    "time_at_evaluation_end = int(datetime.datetime.timestamp(time_at_evaluation_end_datetime))\n",
    "\n",
    "show_hyperparameters(\n",
    "    time_at_evaluation_start=time_at_evaluation_start,\n",
    "    time_at_evaluation_end=time_at_evaluation_end,\n",
    "    flag_show_time_at_evaluation=True\n",
    ")\n",
    "\n",
    "# ===== ヘルパ =====\n",
    "def build_dataset(seed_offset: int = 0):\n",
    "    \"\"\"Dynamic が使える場合は __init__ のシグネチャに合わせて渡す引数を自動フィルタする。\"\"\"\n",
    "    if getattr(conf, \"use_dynamic_action_dataset\", False) and DynamicActionBanditWithTime is not None:\n",
    "        # 候補となる引数（None は後で落とす）\n",
    "        cand = dict(\n",
    "            n_actions=conf.n_actions,\n",
    "            dim_context=conf.dim_context,\n",
    "            n_users=conf.n_users,\n",
    "            t_oldest=conf.t_oldest,\n",
    "            t_now=conf.t_now,\n",
    "            t_future=conf.t_future,\n",
    "            beta=conf.beta,\n",
    "            reward_std=conf.reward_std,\n",
    "            num_time_structure=getattr(conf, \"num_time_structure_for_logged_data\", None),\n",
    "            lambda_ratio=getattr(conf, \"lambda_ratio\", None),\n",
    "            random_state=conf.random_state + seed_offset,\n",
    "        )\n",
    "\n",
    "        # 可用性：関数優先／なければ birth/death\n",
    "        if getattr(conf, \"use_availability_func\", False) and hasattr(conf, \"availability_func_weekly_stair\"):\n",
    "            cand[\"availability_func\"] = conf.availability_func_weekly_stair\n",
    "        else:\n",
    "            if hasattr(conf, \"action_birth_time\"):\n",
    "                cand[\"action_birth_time\"] = conf.action_birth_time\n",
    "            if hasattr(conf, \"action_death_time\") and conf.action_death_time is not None:\n",
    "                cand[\"action_death_time\"] = conf.action_death_time\n",
    "\n",
    "        # None を除去\n",
    "        cand = {k: v for k, v in cand.items() if v is not None}\n",
    "\n",
    "        # ★ __init__ のシグネチャに存在するキーだけ残す（g_coef/h_coef等は自動で落ちる）\n",
    "        params = inspect.signature(DynamicActionBanditWithTime.__init__).parameters\n",
    "        kwargs = {k: v for k, v in cand.items() if k in params}\n",
    "\n",
    "        return DynamicActionBanditWithTime(**kwargs)\n",
    "\n",
    "    # フォールバック：既存の合成データ\n",
    "    return SyntheticBanditWithTimeDataset(\n",
    "        n_actions=conf.n_actions,\n",
    "        dim_context=conf.dim_context,\n",
    "        n_users=conf.n_users,\n",
    "        t_oldest=conf.t_oldest,\n",
    "        t_now=conf.t_now,\n",
    "        t_future=conf.t_future,\n",
    "        beta=conf.beta,\n",
    "        reward_std=conf.reward_std,\n",
    "        num_time_structure=conf.num_time_structure_for_logged_data,\n",
    "        lambda_ratio=conf.lambda_ratio,\n",
    "        flag_simple_reward=conf.flag_simple_reward,\n",
    "        g_coef=conf.g_coef,\n",
    "        h_coef=conf.h_coef,\n",
    "        random_state=conf.random_state + seed_offset,\n",
    "    )\n",
    "\n",
    "def get_avail(dataset, times_vec, feedback=None, fallback_shape=None):\n",
    "    \"\"\"可用マスクの安全取得\"\"\"\n",
    "    if feedback is not None and \"available_actions\" in feedback:\n",
    "        return feedback[\"available_actions\"].astype(bool)\n",
    "    if hasattr(dataset, \"_availability\") and callable(dataset._availability):\n",
    "        return dataset._availability(times_vec).astype(bool)\n",
    "    if fallback_shape is None and feedback is not None and \"expected_reward\" in feedback:\n",
    "        fallback_shape = feedback[\"expected_reward\"].shape\n",
    "    if fallback_shape is None:\n",
    "        raise ValueError(\"fallback_shape を指定してください。\")\n",
    "    return np.ones(fallback_shape, dtype=bool)\n",
    "\n",
    "\n",
    "# ===== 実験（極小） =====\n",
    "start_time = time.time()\n",
    "dataset = build_dataset(seed_offset=seed_offset)\n",
    "\n",
    "# 将来時刻を1つだけサンプル\n",
    "rng = check_random_state(42)\n",
    "t_eval = int(rng.uniform(time_at_evaluation_start, time_at_evaluation_end))\n",
    "\n",
    "# --- 真値近似用 test データ（すべて t_eval） ---\n",
    "test = dataset.obtain_batch_bandit_feedback(\n",
    "    n_rounds=num_test_min,\n",
    "    evaluation_mode=True,\n",
    "    time_at_evaluation=t_eval,\n",
    "    random_state_for_sampling=123\n",
    ")\n",
    "avail_test = get_avail(dataset, np.full(num_test_min, t_eval, dtype=int),\n",
    "                       feedback=test, fallback_shape=test[\"expected_reward\"].shape)\n",
    "\n",
    "# 将来の評価方策（マスクがあれば使う）\n",
    "if gen_eps_greedy_masked is not None and getattr(conf, \"use_masked_policy\", True):\n",
    "    pi_e_test = gen_eps_greedy_masked(\n",
    "        expected_reward=test[\"expected_reward\"],\n",
    "        eps=conf.eps, is_optimal=True,\n",
    "        available_actions=avail_test,\n",
    "    )\n",
    "else:\n",
    "    pi_e_test = gen_eps_greedy(\n",
    "        expected_reward=test[\"expected_reward\"], eps=conf.eps, is_optimal=True\n",
    "    )\n",
    "\n",
    "# 真値 V_t（データセット実装によりマスク渡し可/不可を自動判定）\n",
    "try:\n",
    "    V_true = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=test[\"expected_reward\"],\n",
    "        action_dist=pi_e_test,\n",
    "        available_actions=avail_test,\n",
    "    )\n",
    "except TypeError:\n",
    "    V_true = np.average(test[\"expected_reward\"], weights=pi_e_test, axis=1).mean()\n",
    "\n",
    "# --- 検証データ（ログ）を最小で作成 ---\n",
    "val = dataset.obtain_batch_bandit_feedback(\n",
    "    n_rounds=n_rounds_min, evaluation_mode=False,\n",
    "    random_state_for_sampling=999\n",
    ")\n",
    "pi_b_like = gen_eps_greedy(val[\"expected_reward\"], eps=conf.eps, is_optimal=True)\n",
    "\n",
    "# t_now→t_eval の φ ステップ数（既存 run_ope が要求する引数）\n",
    "days_after = (t_eval - dataset.t_now) // SECONDS_PER_DAY\n",
    "days_per_phi = NUM_DAYS_IN_ONE_CYCLE / dataset.num_time_structure\n",
    "num_phi_steps = int(np.ceil(days_after / days_per_phi))\n",
    "\n",
    "# --- OPE 実行（私用ランナーがあれば masked で、無ければ既存） ---\n",
    "est_list = []\n",
    "for r in range(n_inner_seeds):\n",
    "    run_ope_fn(\n",
    "        dataset=dataset,\n",
    "        round=r,\n",
    "        time_at_evaluation=t_eval,\n",
    "        estimated_policy_value_list=est_list,\n",
    "        val_bandit_data=val,\n",
    "        action_dist_val=pi_b_like,\n",
    "        num_true_time_structure_for_OPFV_reward=conf.num_true_time_structure_for_OPFV_reward,\n",
    "        num_true_time_structure_for_OPFV_for_context=None,\n",
    "        num_episodes_for_Prognosticator=conf.num_episodes_for_Prognosticator,\n",
    "        num_time_structure_from_t_now_to_time_at_evaluation=num_phi_steps,\n",
    "        eps=conf.eps,\n",
    "        true_policy_value=V_true,\n",
    "        flag_Prognosticator_optimality=conf.flag_Prognosticator_optimality,\n",
    "        num_features_for_Prognosticator_list=conf.num_features_for_Prognosticator_list,\n",
    "        flag_include_DM=conf.flag_include_DM,\n",
    "        flag_calculate_data_driven_OPFV=conf.flag_calculate_data_driven_OPFV,\n",
    "        candidate_num_time_structure_list=conf.candidate_num_time_structure_list,\n",
    "    )\n",
    "\n",
    "# === 結果表示（小さな表だけ） ===\n",
    "df = DataFrame(DataFrame(est_list).stack()).reset_index(1).rename(columns={\"level_1\": \"est\", 0: \"value\"})\n",
    "df[\"SE\"] = (df[\"value\"] - V_true) ** 2\n",
    "print(\"t_eval:\", datetime.datetime.fromtimestamp(t_eval), \"| n_rounds:\", n_rounds_min, \"| num_test:\", num_test_min)\n",
    "print(\"True V_t:\", V_true)\n",
    "display(df)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"elapsed: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.155\n"
     ]
    }
   ],
   "source": [
    "# 将来 t' の不可用が存在するラウンド割合（>0 なら可用変化あり）\n",
    "eval_unavail_frac = np.mean(avail_test.sum(axis=1) < dataset.n_actions)\n",
    "\n",
    "# 将来方策が不可用行動に載せた確率の合計（理想は 0）\n",
    "mass_on_unavail = float(np.max(np.sum(pi_e_test * (~avail_test), axis=1)))\n",
    "\n",
    "# ログ側にも不可用がある？（Dynamic なら >0 が普通）\n",
    "log_unavail_frac = np.mean(dataset._availability(val[\"time\"]).sum(axis=1) < dataset.n_actions) \\\n",
    "                   if hasattr(dataset, \"_availability\") else 0.0\n",
    "\n",
    "print(eval_unavail_frac, mass_on_unavail, log_unavail_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taish\\anaconda3\\envs\\cfml\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:8246: RuntimeWarning: invalid value encountered in power\n",
      "  g1 = mu3 / np.power(mu2, 1.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_unavail_frac: 1.0 | mass_on_unavail: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>est</th>\n",
       "      <th>value</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DR</td>\n",
       "      <td>13.698106</td>\n",
       "      <td>16.638305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPS</td>\n",
       "      <td>13.048208</td>\n",
       "      <td>11.758799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPFV</td>\n",
       "      <td>9.813020</td>\n",
       "      <td>0.037606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPFV-masked</td>\n",
       "      <td>9.813020</td>\n",
       "      <td>0.037606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prognosticator</td>\n",
       "      <td>13.048208</td>\n",
       "      <td>11.758799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V_t</td>\n",
       "      <td>9.619098</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data-driven OPFV</td>\n",
       "      <td>10.427050</td>\n",
       "      <td>0.652787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                est      value         SE\n",
       "0                DR  13.698106  16.638305\n",
       "0               IPS  13.048208  11.758799\n",
       "0              OPFV   9.813020   0.037606\n",
       "0       OPFV-masked   9.813020   0.037606\n",
       "0    Prognosticator  13.048208  11.758799\n",
       "0               V_t   9.619098   0.000000\n",
       "0  data-driven OPFV  10.427050   0.652787"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 既存の dataset / val / run_ope_fn がある前提（無ければ最小実験セルの前段を流してください）\n",
    "SECONDS_PER_DAY = 86400\n",
    "\n",
    "# 1) t' を “登場がまだ途中” のタイミングに固定（例：最古時刻 + 2週間）\n",
    "t_eval_early = int(dataset.t_oldest + 2 * 7 * SECONDS_PER_DAY)\n",
    "\n",
    "# 将来の可用マスクと将来方策（マスク付き）\n",
    "test_early = dataset.obtain_batch_bandit_feedback(\n",
    "    n_rounds=1000, evaluation_mode=True, time_at_evaluation=t_eval_early, random_state_for_sampling=123\n",
    ")\n",
    "avail_test_early = dataset._availability(np.full(1000, t_eval_early)).astype(bool) if hasattr(dataset, \"_availability\") else np.ones_like(test_early[\"expected_reward\"], bool)\n",
    "\n",
    "from policy import gen_eps_greedy, gen_eps_greedy_masked\n",
    "pi_e_test_early = gen_eps_greedy_masked(\n",
    "    expected_reward=test_early[\"expected_reward\"], eps=conf.eps, is_optimal=True, available_actions=avail_test_early\n",
    ")\n",
    "\n",
    "# どのくらい“まだ不可用”がある？\n",
    "eval_unavail_frac = float(np.mean(avail_test_early.sum(axis=1) < dataset.n_actions))\n",
    "mass_on_unavail   = float(np.max(np.sum(pi_e_test_early * (~avail_test_early), axis=1)))\n",
    "print(\"eval_unavail_frac:\", eval_unavail_frac, \"| mass_on_unavail:\", mass_on_unavail)\n",
    "\n",
    "# 2) 最小のログを作ってランナー実行\n",
    "val_small = dataset.obtain_batch_bandit_feedback(n_rounds=200, evaluation_mode=False, random_state_for_sampling=999)\n",
    "\n",
    "# 参考：将来真値\n",
    "try:\n",
    "    V_true_early = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=test_early[\"expected_reward\"],\n",
    "        action_dist=pi_e_test_early,\n",
    "        available_actions=avail_test_early,\n",
    "    )\n",
    "except TypeError:\n",
    "    V_true_early = np.average(test_early[\"expected_reward\"], weights=pi_e_test_early, axis=1).mean()\n",
    "\n",
    "from pandas import DataFrame\n",
    "est_list = []\n",
    "run_ope_fn(\n",
    "    dataset=dataset,\n",
    "    round=0,\n",
    "    time_at_evaluation=t_eval_early,\n",
    "    estimated_policy_value_list=est_list,\n",
    "    val_bandit_data=val_small,\n",
    "    action_dist_val=gen_eps_greedy(val_small[\"expected_reward\"], is_optimal=True, eps=conf.eps),\n",
    "    num_true_time_structure_for_OPFV_reward=conf.num_true_time_structure_for_OPFV_reward,\n",
    "    num_true_time_structure_for_OPFV_for_context=None,\n",
    "    num_episodes_for_Prognosticator=conf.num_episodes_for_Prognosticator,\n",
    "    num_time_structure_from_t_now_to_time_at_evaluation=1,\n",
    "    eps=conf.eps,\n",
    "    true_policy_value=V_true_early,\n",
    "    flag_Prognosticator_optimality=conf.flag_Prognosticator_optimality,\n",
    "    num_features_for_Prognosticator_list=conf.num_features_for_Prognosticator_list,\n",
    "    flag_include_DM=conf.flag_include_DM,\n",
    "    flag_calculate_data_driven_OPFV=conf.flag_calculate_data_driven_OPFV,\n",
    "    candidate_num_time_structure_list=conf.candidate_num_time_structure_list,\n",
    ")\n",
    "df = DataFrame(DataFrame(est_list).stack()).reset_index(1).rename(columns={\"level_1\":\"est\", 0:\"value\"})\n",
    "df[\"SE\"] = (df[\"value\"] - V_true_early) ** 2\n",
    "display(df.sort_values(\"est\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
